#!/usr/bin/env python3

import os
import sys
import re
from glob import glob

import argparse
from samflow.command import ShellCommand, PythonCommand
from samflow.workflow import Workflow, attach_back
from pkg_resources import resource_filename

from gcap.config import Conf
from gcap.helpers import *

## hotspot
token_file = resource_filename("gcap", "static/runall.tokens.txt")
pipeline_scripts = resource_filename("gcap", "pipeline-scripts")

## latex template
tex = resource_filename("gcap", "static/gcap_template.tex")

def parse_args(args=None):
    """
    If args is None, argparse will parse from sys.argv
    """
    description = "GCAP :  Global Chromatin Accessibility Pipeline"
    parser = argparse.ArgumentParser(description=description)
    sub_parsers = parser.add_subparsers(help="sub-command help", dest="sub_command")

    parser_run = sub_parsers.add_parser("run", help="run pipeline using a config file",
        description="GCAP-run: Run GCAP pipeline using a config file")

    parser_batch = sub_parsers.add_parser("batch", help="run pipeline for multiple datasets")
    parser_batch.add_argument("-b", "--batch-config", dest="batch_config", required=True, help="batch file")
    parser_purge = sub_parsers.add_parser("purge")
    parser_clean = sub_parsers.add_parser("clean", help="Move result file into a new folder and delete other files",
        description="GCAP-run: Run GCAP pipeline using a config file")

    for p in (parser_run, parser_batch):
        p.add_argument("--from", dest="start_step", default=0, type=int,
            help="Only step after this number will be processed")
        p.add_argument("--to", dest="end_step", default=100, type=int,
            help="Only step before this number will be processed ")
        p.add_argument("--skip", dest="skip_step", default="",
            help="Steps to skip, use comma as seperator")

    for p in (parser_run, parser_batch, parser_purge, parser_clean):
        p.add_argument("-v", "--verbose-level", dest="verbose_level", type=int, default=2)
        p.add_argument("--dry-run", dest="dry_run", action="store_true", default=False)
        p.add_argument("--allow-dangling", dest="allow_dangling", action="store_true", default=False)
        p.add_argument("--resume", dest="resume", action="store_true", default=False)
        p.add_argument("--remove", dest="clean", action="store_true", default=False)

    for p in (parser_run, parser_purge, parser_clean):
        p.add_argument("-c", "--config", dest="config", required=True,
            help="specify the config file to use", )

    return parser.parse_args(args)

def _sample_reads(workflow, conf, N, format):
    """
    sample 100k from fastq file for library contamination and sequence quality evaluation
    sample 5M reads from sam files for other evaluation to avoid remapp
    """
    if N == 100000:
        suffix = "100k"
    elif N == 5000000:
        suffix = "5M"
    else:
        print("This sampling depth is not used!")
    if format == "fastq":
        if conf.seq_type == "se":
            for n, target in enumerate(conf.treatment_targets):
                attach_back(workflow,
                    PythonCommand(single_end_fastq_sampling,
                        input={"fastq": conf.treatment_raws[n]},
                        output={"fastq_sample": target + "_%s.fq" % suffix},
                        param={"random_number": N}))     ## use 100kb random reads
        elif conf.seq_type == "pe":
            for raw, target in conf.treatment_pairs_pe:
                attach_back(workflow,
                    PythonCommand(pair_end_fastq_sampling,
                    input={"fastq": raw},
                    output={"fastq_sample": [ i + "_%s.fq" % suffix for i in target ]},
                    param={"random_number": N}))     ## use 100kb random reads

    elif format == "sam":
        for target, s in zip(conf.treatment_targets, conf.treatment_bases):
            attach_back(workflow, ShellCommand(
               "{tool} {input[sam]} {param[prog]} {output[bam]}",
                tool = "sampling_sam_by_num.sh",
                input = {"sam": target + ".sam.all"},
                output = {"bam": target + ".sam.%s" % suffix},
                param = {"prog": conf.get("picard", "sample")}))

def _fastqc(workflow, conf):
    """
    sample 100k reads for fastqc evaluation
    ## TODO: replace fastqc by using Jim's codes
    """

    ## sample 100k reads
    _sample_reads(workflow, conf, 100000, "fastq")

    ## for single end and pair end data
    if conf.seq_type == "se":
        for target in conf.treatment_targets:
                fastqc_run = attach_back(workflow,
                    ShellCommand(
                        "{tool} {input[fastq_sample]} --extract -t {param[threads]} -o {output[target_dir]}",
                        input= {"fastq_sample": target + "_100k.fq"},
                        output={"target_dir": conf.target_dir,
                                "fastqc_summary": target + "_100k.fq_fastqc/fastqc_data.txt"},
                        tool="fastqc",
                        param={"threads": 4}))
                fastqc_run.update(param=conf.items("fastqc"))
        ## get per sequence quality median
        attach_back(workflow, PythonCommand(stat_fastqc,
                input = {"fastqc_summaries": [ t + "_100k.fq_fastqc/fastqc_data.txt" for t in conf.treatment_targets ]},
                output = {"json": conf.json_prefix + "_fastqc.json"},
                param = {"samples": conf.treatment_bases}))

        ## sequence quality latex load, reads length
        attach_back(workflow, PythonCommand(
            fastqc_doc,
            input = {"tex": tex, "json": conf.json_prefix + "_fastqc.json"},
            output = {"seq": conf.latex_prefix + "seq_quality.tex", "len": conf.latex_prefix + "len.tex"},
            param = {"seq_type": conf.seq_type, "reps": len(conf.treatment_pairs), "se_samples": conf.treatment_bases}))

    elif conf.seq_type == "pe":
        for n, target in enumerate(conf.treatment_pair_data):
            for p in target:
                fastqc_run = attach_back(workflow,
                    ShellCommand(
                        "{tool} {input[fastq_sample]} --extract -t {param[threads]} -o {output[target_dir]}",
                        input= {"fastq_sample": p + "_100k.fq"},
                        output={"target_dir": conf.target_dir,
                                "fastqc_summary": p + "_100k.fq_fastqc/fastqc_data.txt"},
                        tool="fastqc",
                        param={"threads": 4}))

                fastqc_run.update(param=conf.items("fastqc"))

            attach_back(workflow, PythonCommand(stat_fastqc,
                input = {"fastqc_summaries": [ p + "_100k.fq_fastqc/fastqc_data.txt" for p in target ]},
                output = {"json": conf.json_prefix + "_rep" + str(n+1) + "_fastqc.json"},
                param = {"samples": target }))

        ## sequence quality latex load, reads length
        attach_back(workflow, PythonCommand(
            fastqc_doc,
            input = {"tex": tex, "json": [ conf.json_prefix + "_rep" + str(n+1) + "_fastqc.json" for n in range(len(conf.treatment_pair_data)) ]},
            output = {"seq": conf.latex_prefix + "seq_quality.tex", "len": conf.latex_prefix + "len.tex"},
            param = {"seq_type": conf.seq_type, "reps": len(conf.treatment_pairs),
                     "pe_samples": [ target for target in conf.treatment_pair_data ]}))

def _lib_contamination(workflow, conf):
    """ input PE and SE sampled data 100 K to run,
     output json for library contamination, if not correct, shutdown
    """
    ## bowtie mapping back to mouse, rat and human
    if conf.seq_type == "se":
        for target in conf.treatment_targets:
            for species in dict(conf.items("contaminate")):
                attach_back(workflow,
                    ShellCommand(
                        "{tool} -p {param[threads]} -S -m {param[max_align]} \
                        {param[genome_index]} {input[fastq_sample]} {output[sam]} 2> {output[bowtie_summary]}",
                        input={"fastq_sample": target + "_100k.fq"},
                        output={"sam": target + ".sam.100k",
                                "bowtie_summary": target + species + "_contam_summary.txt"},
                        tool="bowtie",
                        param={"threads": 4,
                               "max_align": 1,
                               "genome_index": conf.get_path("contaminate", species)}))

    elif conf.seq_type == "pe":
        for n, target in enumerate(conf.treatment_pair_data):
            for species in dict(conf.items("contaminate")):
                attach_back(workflow,
                    ShellCommand(
                        "{tool} -X 600 --chunkmbs 300 -n 1 -p {param[threads]} -S -m {param[max_align]} \
                        {param[genome_index]} -1 {input[fastq_sample][0]} -2 {input[fastq_sample][1]} {output[sam]} 2> {output[bowtie_summary]}",
                        input={"fastq_sample": [ t + "_100k.fq" for t in target ]},
                        output={"sam": conf.treatment_targets[n] + ".sam.100k",
                                "bowtie_summary": conf.treatment_targets[n] + species + "_contam_summary.txt"},
                        tool="bowtie",
                        param={"threads": 4,
                               "max_align": 1,
                               "genome_index": conf.get_path("contaminate", species)}))
    all_species = [i for i, _ in conf.items("contaminate")]
    bowtie_summaries = []
    for target in conf.treatment_targets:
        bowtie_summaries.append([target + species + "_contam_summary.txt" for species in all_species])

    ## if library is contaminated, breaks
    attach_back(workflow,
        PythonCommand(stat_contamination,
            input={"bowtie_summaries": bowtie_summaries},
            output={"json": conf.json_prefix + "_contam.json"},
            param={"samples": conf.treatment_bases,
                   "id": conf.id,
                   "species": all_species,
                   "correct_species": conf.get("Basis", "species")}))

def _bowtie(workflow, conf):
    """
     all reads mapping
    """
    if conf.seq_type == "se":
        for raw, target in conf.treatment_pairs:
            bowtie = attach_back(workflow,
                            ShellCommand(
                                "{tool} -n 1 -p {param[threads]} -S -m {param[max_align]} \
                                {param[genome_index]} {input[fastq]} {output[sam]} 2> {output[bowtie_summary]}",
                                input={"fastq": raw},
                                output={"sam": target + ".sam.all",
                                        "bowtie_summary": target + "all_um_summary.txt"},
                                tool="bowtie",
                                param={"threads": 4,
                                       "max_align": 1,
                                       "genome_index": conf.get_path("lib", "genome_index")}))
            bowtie.update(param = conf.items("bowtie"))
    elif conf.seq_type == "pe":
        for n, target in enumerate(conf.treatment_targets):
            bowtie = attach_back(workflow,
                        ShellCommand(
                            "{tool} -X 600 --chunkmbs 300 -n 1 -p {param[threads]} -S -m {param[max_align]} \
                            {param[genome_index]} -1 {input[fastq][0]} -2 {input[fastq][1]} {output[sam]} 2> {output[bowtie_summary]}",
                            input={"fastq": conf.treatment_raws[n]},
                            output={"sam": target + ".sam.all",
                                    "bowtie_summary": target + "all_um_summary.txt"},
                            tool="bowtie",
                            param={"threads": 4,
                                   "max_align": 1,
                                   "genome_index": conf.get_path("lib", "genome_index")}))
            bowtie.update(param = conf.items("bowtie"))


    ## filter mitochondria tags
    uniq_tag = resource_filename("gcap", "pipeline-scripts/uniq_map_exclude_chrM.sh")

    for target in conf.treatment_targets:

        attach_back(workflow, ShellCommand(
            "{tool} -f {param[script]} {input[sam]} > {output[count]}",
            tool = "awk",
            input = {"sam": target + ".sam.all"},
            output = {"count": target + "_uniq_tag_autosome.count"},
            param = {"script": uniq_tag}))

    attach_back(workflow, PythonCommand(
            autosome_map,
            input = {"count": [ target + "_uniq_tag_autosome.count" for target in conf.treatment_targets ]},
            output = {"json": conf.json_prefix + "_um_autosome.json"}, param = {"samples": conf.treatment_bases}))

    bowtie_summaries = []
    for target in conf.treatment_targets:
        bowtie_summaries.append(target + "all_um_summary.txt")

    attach_back(workflow,
        PythonCommand(stat_bowtie,
            input={"bowtie_summaries": bowtie_summaries},
            output={"json": conf.json_prefix + "_mapping.json"},
            param={"sams": conf.treatment_bases}))

## optionally
def _bwa(workflow, conf):
    """bwa aln /homea6/qinqian/lib/hg19.fa HL1_LNcap_DHT_50U_50_300bp_CTTGTA_L004_R1_001.fastq > HL1_LNcap_DHT_50U_50_300bp_CTTGTA_L004_R1_001.sai"""
    for target in conf.sample_targets:
        attach_back(workflow, ShellCommand(
            "{tool} aln {input[index]} {input[fastq]} > {output[sai]}",
            tool = "bwa",
            input = {"index": conf.get("lib", "genome_index"), "fastq": conf.sample_targets}))


def _reads_mapping(workflow, conf):
    """
    reads mapping for all reads
    and convert sam to bam, sort
    """
    if conf.maptool == "bowtie":
        _bowtie(workflow, conf)
    elif conf.maptool == "bwa":
        _bwa(workflow, conf)
    else:
        print("Not support mapping tool")
    ## SortSam to convert and sort for all reads
    for target in conf.treatment_targets:
        lib_sort = attach_back(workflow, ShellCommand(
            "{tool} -Xmx2g -XX:ParallelGCThreads={param[threads]} -jar {param[sort]} I={input[bam]} O={output[bam]} SO=coordinate \
            VALIDATION_STRINGENCY=SILENT",
            tool = "java",
            input = {"bam": target + ".sam.all"},
            output = {"bam": target + "_picard_sort.bam.all"},
            param = {"threads": 4}))
        lib_sort.update(param = conf.items("picard"))

    ## raw reads and reads mapping qc
    attach_back(workflow, PythonCommand(
            reads_doc,
            input = {"tex": tex, "json": conf.json_prefix + "_mapping.json"},
            output = {"raw": conf.latex_prefix + "raw.tex",
                      "mapping": conf.latex_prefix + "mapping.tex"},
            param = {"seq_type": conf.seq_type,
                     "reps": len(conf.treatment_pairs),
                     "samples": conf.treatment_bases}))

def _library_complexity(workflow, conf):
    """ use 5M reads, those are not primary alignment reads are discarded
     to estimate complexity => duplicates
    ## TODO: replace picard by using Gifford's script of Census
    """
    ## sampling 5M from all reads mapping sam files using picard

    _sample_reads(workflow, conf, 5000000, "sam")

    # picard markduplicates need SortSam
    for target in conf.treatment_targets:
        lib_sort = attach_back(workflow, ShellCommand(
            "{tool} -Xmx2g -XX:ParallelGCThreads={param[threads]} -jar {param[sort]} I={input[bam]} O={output[bam]} SO=coordinate \
            VALIDATION_STRINGENCY=SILENT",
            tool = "java",
            input = {"bam": target + ".sam.5M"},
            output = {"bam": target + "_picard_sort.bam.5M"},
            param = {"threads": 4}))
        lib_sort.update(param = conf.items("picard"))

    # markduplicates
    for target in conf.treatment_targets:
        lib_dup = attach_back(workflow, ShellCommand(
            "{tool} -Xmx2g -XX:ParallelGCThreads={param[threads]} -jar {param[markdup]} I={input[bam]} O={output[bam]} METRICS_FILE={output[metrics]} REMOVE_DUPLICATES=false \
            VALIDATION_STRINGENCY=SILENT",
            tool = "java",
            input = {"bam": target + "_picard_sort.bam.5M"},
            output = {"bam": target + "_markdup.bam.5M", "metrics": target + "_markdup_metric.5M"},
            param = {"threads": 4}))
        lib_dup.update(param = conf.items("picard"))


    attach_back(workflow, PythonCommand(
            stat_redun,
            input = {"picard": [target + "_markdup_metric.5M" for target in conf.treatment_targets ]},
            output = {"json": conf.json_prefix + "_redun.json"}, param = {"samples": conf.treatment_bases}))

    attach_back(workflow, PythonCommand(
            redundancy_doc,
            input = {"tex": tex, "json": conf.json_prefix + "_redun.json"},
            output = {"redun": conf.latex_prefix + "redun.tex"},
            param = {"reps": len(conf.treatment_pairs), "samples": conf.treatment_bases}))

def _fragment_size(workflow, conf):
    """
    We use MACS2 for mean fragment size and standard deviation estimation
    Use picard for PE mode temporarily
    Use macs2 for SE mode, bug: several fragment sizes available
    """
    for target in conf.treatment_targets:
        fragment_size = attach_back(workflow, ShellCommand(
                                "{tool} predictd -i {input[bam]} --rfile {param[prefix]}",
                                tool = "macs2",
                                input = {"bam": target + "_picard_sort.bam.5M"},
                                output = {"R": target + "_5M_model.R"},
                                param = {"prefix": target + "_5M"}))

    ## extract standard deviation from MACS2 model.R,
    ## use m, p, and pileup value for standard deviation; mean fragment size is provided.
    ## TODO: need to improve
    attach_back(workflow, PythonCommand(
        stat_frag_std,
        input = {"r": [ target + "_5M_model.R" for target in conf.treatment_targets ]},
        output = {"json": conf.json_prefix + "_frag.json", "r": [ target + "_5M_frag_sd.R" for target in conf.treatment_targets ] },
        param = {"samples": conf.treatment_bases}))

    attach_back(workflow, PythonCommand(
        frag_doc,
        input = {"json": conf.json_prefix + "_frag.json", "tex": tex},
        output = {"latex": conf.latex_prefix + "_frag.tex"},
        param = {"reps": len(conf.treatment_pairs),
                 "samples": conf.treatment_bases}))

def _hotspot_on_replicates(workflow, conf):
    # I write a tags.sh in pipeline-scripts to convert BAM files to 1 bp BED files
    # for both SE and PE using all and 5M reads
    ## all reads, need to get 1bp tags density, and 20bp tags density to bigwiggle
    ## use two passes merge peaks and hotspot to estimate peaks number
    ## sometimes wavepeaks don't get peaks evenly distributed, shut down chromosome check

    for i, target in enumerate(conf.treatment_targets):
        ## prepare input for hotspot, get mappable tags and starch into output directory
        attach_back(workflow,
            ShellCommand(
                "{tool} {input} {output} {param[type]}",
                tool = "tags.sh",
                input = target + "_picard_sort.bam.all",
                output = target + ".bed.starch",
                param = {"type": conf.seq_type}))
        ## generate configuration for hotspot v3
        hotspot_conf = attach_back(workflow,
            PythonCommand(spot_conf,
                input = {"spot_conf": token_file,
                         "tag": target + ".bed.starch",
                         "mappable_region": conf.get("hotspot", "mappable_region"),
                         "chrom_info": conf.get("hotspot", "chrom_info")},
                output = {"conf": target + "_runall.tokens.txt",
                          "dir": conf.target_dir},
                param = {"fdrs": "0.01", "K": conf.get("Basis", "read_length"),
                         "species": conf.get("Basis", "species")}))
        hotspot_conf.param.update(conf.items("hotspot"))
        ## run hotspot v3
        attach_back(workflow, ShellCommand(
            "{tool} {input[pipe]} {input[token]} {output[dir]}",
            tool = "runhotspot",
            input = {"pipe": pipeline_scripts, "token": target + "_runall.tokens.txt"},
            output = {"dir": conf.target_dir,
                      "spot_peaks_combined": conf.hotspot_reps_both_passes_prefix[i] + ".hotspot.twopass.fdr0.01.merge.pks.bed",
                      "density_starch": target + ".tagdensity.bed.starch",
                      "spot": target + ".spot.out"},
            param = None))

    ## all reads, calculate the merged two passes and peaks regions number


    ## Use 5M reads to estimate SPOT
    for i, target in enumerate(conf.treatment_targets):
        ## prepare input for hotspot, get mappable tags and starch into output directory
        attach_back(workflow,
            ShellCommand(
                "{tool} {input} {output} {param[type]}",
                tool = "tags.sh",
                input = target + "_picard_sort.bam.5M",
                output = target + "_5M.bed.starch",
                param = {"type": conf.seq_type}))
        ## generate configuration for hotspot v3
        hotspot_conf = attach_back(workflow,
            PythonCommand(spot_conf,
                input = {"spot_conf": token_file,
                         "tag": target + "_5M.bed.starch",
                         "mappable_region": conf.get("hotspot", "mappable_region"),
                         "chrom_info": conf.get("hotspot", "chrom_info")},
                output = {"conf": target + "_runall_5M.tokens.txt",
                          "dir": conf.target_dir},
                param = {"fdrs": "0.01", "K": conf.get("Basis", "read_length"),
                         "species": conf.get("Basis", "species")}))
        hotspot_conf.param.update(conf.items("hotspot"))
        ## run hotspot v3
        attach_back(workflow, ShellCommand(
            "{tool} {input[pipe]} {input[token]} {output[dir]}",
            tool = "runhotspot",
            input = {"pipe": pipeline_scripts, "token": target + "_runall_5M.tokens.txt"},
            output = {"dir": conf.target_dir,
                      "spot_peaks_combined": conf.hotspot_reps_both_passes_5M_prefix[i] + ".hotspot.twopass.fdr0.01.merge.pks.bed",
                      "density_starch": target + "_5M.tagdensity.bed.starch",
                      "spot": target + "_5M.spot.out"},
            param = None))

        ## density from hotspot 3, 20bp resolution
        ## for correlation evaluation
        attach_back(workflow, ShellCommand(
            "{tool} {input[starch]} > {output[bed]}",
            tool = "unstarch",
            input = {"starch": target + "_5M.tagdensity.bed.starch"},
            output = {"bed": target + "_5M.tagdensity.bed.tmp"}))

        ## For bedGraphToBigwiggle bugs, we need to remove coordinates outlier
        ## filter bdg file to remove over-border coordinates
        ## with bedtools or bedClip
        attach_back(workflow,
            ShellCommand(
                '{tool} intersect -a {input} -b {param[chrom_bed]} -f 1.00 > {output}',
                tool="bedtools",
                input=target + "_5M.tagdensity.bed.tmp",
                output=target + "_5M.tagdensity.bed",
                param={'chrom_bed': conf.get("lib", "chrom_bed")},
                name="bed replicate filtering"))
        ## convert to bigwiggle
        attach_back(workflow,
            ShellCommand(
                'cut -f 1,2,3,5 {input} > {input}.tmp && {tool} {input}.tmp {param[chrom_len]} {output}',
                tool = "bedGraphToBigWig",
                input=target + "_5M.tagdensity.bed",
                output=target + "_5M.bw",
                param={"chrom_len": conf.get("lib", "chrom_len")}, name="bdg_to_bw"))


def _peaks_reps_preprocess(workflow, conf):
    """
    remove blacklist and outlier
    """

    has_velcro = conf.get("lib", "velcro")  ## mouse not have blacklist yet
    for i, target in enumerate(conf.treatment_targets):
        ## remove blacklist for human
        ## mouse no blacklist ??
        if conf.peakcalltool == "hotspot":
            input_peaks = conf.hotspot_reps_both_passes_5M_prefix[i] + ".hotspot.twopass.fdr0.01.merge.pks.bed"
            non_velcro = target + "_5M_velcro_non_overlap_peaks.bed"
        elif conf.peakcalltool == "macs2":
            input_peaks = target + "_5M_macs2_peaks.bed"
            non_velcro = target + "_5M_macs2_velcro_non_overlap_peaks.bed"
        if has_velcro:
            attach_back(workflow,
                ShellCommand(
                    "{tool} -v -a {input[peaks]} -b {input[velcro_peaks_bed]} > {output}",
                    tool="intersectBed",
                    input={"peaks": input_peaks,
                           "velcro_peaks_bed": conf.get("lib", "velcro")},
                    output = non_velcro,
                    name = "filter blacklist",
                    param=None))
        else: ## for mouse, just rename to *_5M_velcro_non_overlap_peaks.bed
            attach_back(workflow,
                ShellCommand(
                    "cp {input[peaks]} {output}",
                    tool="cp",
                    input={"peaks": input_peaks},
                    output = non_velcro,
                    name = "No blacklist, not filter blacklist",
                    param=None))

        ## awk and bedClip to remove outlier location from above input
        attach_back(workflow,
            ShellCommand(
                "{tool} '{{if ($2 >= 0 && $2 < $3) print}}' {input} > {output}",
                tool="awk",
                input = non_velcro,
                output = non_velcro + ".tmp",
                name = "filter bed files outlier location"))

        # prototype used here to do the similar thing on bedclip
        bed_clip = attach_back(workflow,
            ShellCommand(
                template="{tool} {input} {param[chrom_len]} {output}",
                tool="bedClip",
                input=non_velcro + ".tmp",
                output = non_velcro + ".final",
                param={'chrom_len': conf.get_path("lib", "chrom_len")},
                name="bedclip filter"))
        bed_clip.allow_fail = True

def _peaks_reps_evaluating(workflow, conf):
    ## use 5M reads to evaluate
    ## intersect region ratio in pairwise ways
    for i in range(len(conf.treatment_targets)):
        for j in range(i+1, len(conf.treatment_targets)):
            if conf.peakcalltool == "hotspot":
                one_rep = conf.treatment_targets[i] + "_5M_velcro_non_overlap_peaks.bed.final"
                another_rep = conf.treatment_targets[j] + "_5M_velcro_non_overlap_peaks.bed.final"
            elif conf.peakcalltool == "macs2":
                one_rep = conf.treatment_targets[i] + "_5M_macs2_velcro_non_overlap_peaks.bed.final"
                another_rep =  conf.treatment_targets[j] + "_5M_macs2_velcro_non_overlap_peaks.bed.final"
            attach_back(workflow,
                ShellCommand(
                    "{tool} -a {input[one_rep]} -b {input[another_rep]} > {output}",
                    tool="intersectBed",
                    input = {"one_rep": one_rep,
                             "another_rep": another_rep},
                    output= conf.treatment_targets[i] + "_reps_overlap_" + "rep" + str(i+1) + "rep" + str(j+1) + ".bed",
                    name = "replicates Overlap",
                    param=None))

    ## get union peaks region from pairwise replicates
    bed_merge = attach_back(workflow, ShellCommand(
            "cat {param[reps]} | sort-bed - | {tool} -m - > {output}",
            tool = "bedops",
            input = {"reps": [ target + "_5M_velcro_non_overlap_peaks.bed.final" if conf.peakcalltool == "hotspot" else target + "_5M_macs2_velcro_non_overlap_peaks.bed.final"
                               for target in conf.treatment_targets ]},
            output = conf.prefix + "_5M_reps_union.bed"))
    bed_merge.param.update({"reps": " ".join(bed_merge.input["reps"])})

    ## Implemented by Jim, correlation on union regions
    ## bigWigCorrelate_2 -restrict=testid_reps_union_region.bb testid_treat_rep1_5M.bw testid_treat_rep2_5M.bw
    ## if you want genome wide correlation, use wigCorrelate
    union = conf.prefix + "_5M_reps_union.bed"
    bed_clip = attach_back(workflow,
        ShellCommand(
            template="{tool} {input} {param[chrom_len]} {output}",
            tool="bedClip",
            input=union,
            output =union + ".filter",
            param={'chrom_len': conf.get_path("lib", "chrom_len")},
                  name="bedclip filter"))

    for i in range(len(conf.treatment_targets)):
        for j in range(i+1, len(conf.treatment_targets)):
            if conf.peakcalltool == "macs2":
                one_rep = conf.treatment_targets[i] + "_5M_macs2_treat.bw"
                another_rep = conf.treatment_targets[j] + "_5M_macs2_treat.bw"
            elif conf.peakcalltool == "hotspot":
                one_rep = conf.treatment_targets[i] + "_5M.bw"
                another_rep = conf.treatment_targets[j] + "_5M.bw"


            attach_back(workflow,
                ShellCommand(
                    "bedToBigBed {input[union]} {param[chrom_bed]} {output[bb]} && {tool} -restrict={output[bb]} {input[one_rep]} {input[another_rep]} 1>{output[cor_score]}",
                    tool="bigWigCorrelate",
                    input = {"one_rep": one_rep,
                             "another_rep": another_rep,
                             "union": union + ".filter"},
                    output= {"cor_score": conf.treatment_targets[i] + "_reps_" + str(i+1) + str(j+1) + "cor_score",
                             "bb": conf.treatment_targets[i] + ".bigbed"},
                    name = "replicates bigwiggle correlation",
                    param = {"chrom_bed": conf.get_path("lib", "chrom_len")}))

    ## peaks replicates json
    bed = [ (conf.treatment_targets[i] + "_reps_overlap_" + "rep" + str(i+1) + "rep" + str(j+1) + ".bed", "Rep %s %s" %(i + 1, j + 1))
            for i in range(len(conf.treatment_targets)) for j in range(i+1, len(conf.treatment_targets)) ]

    cor = [ (conf.treatment_targets[i] + "_reps_" + str(i+1) + str(j+1) + "cor_score", "Rep %s %s" %(i + 1, j + 1))
            for i in range(len(conf.treatment_targets)) for j in range(i+1, len(conf.treatment_targets)) ]

    ## get output to json
    attach_back(workflow, PythonCommand(
        stat_reps,
        input = {"5M_overlap": bed, "5M_cor": cor, "union": union},
        output = {"json": conf.json_prefix + "_reps.json"}))

    ## latex document
    attach_back(workflow, PythonCommand(
        reps_doc,
        input = {"json": conf.json_prefix + "_reps.json", "tex": tex},
        output = {"latex": conf.latex_prefix + "_reps.tex"},
        param = {"samples": conf.treatment_bases}))

def _hotspot_combo(workflow, conf):
    ## starchcat all replicates
    ## all reads combo
    attach_back(workflow,
        ShellCommand(
            "{tool} {param[tag_list]} > {param[tmp]} && unstarch {param[tmp]} | sort-bed - | starch - > {output}",
            tool = "starchcat",
            input = [ target + ".bed.starch" for target in conf.treatment_targets ],
            output = conf.prefix + "_merge_all.bed.starch",
            param = {"tag_list": " ".join([ target + ".bed.starch" for target in conf.treatment_targets ]),
                     "tmp": conf.prefix + "_merge_all.bed.starch.tmp"}))

    ## config for hotspot 3
    hotspot_conf = attach_back(workflow,
        PythonCommand(spot_conf,
            input = {"spot_conf": token_file,
                     "tag": conf.prefix + "_merge_all.bed.starch",
                     "mappable_region": conf.get("hotspot", "mappable_region"),
                     "chrom_info": conf.get("hotspot", "chrom_info")},
            output = {"conf": conf.prefix + "_runall_all.tokens.txt",
                      "dir": conf.target_dir},
            param = {"fdrs": "0.01", "K": conf.get("Basis", "read_length"),
                     "species": conf.get("Basis", "species")}))
    hotspot_conf.param.update(conf.items("hotspot"))

    ## run hotspot v3
    attach_back(workflow, ShellCommand(
        "{tool} {input[pipe]} {input[token]} {output[dir]}",
        tool = "runhotspot",
        input = {"pipe": pipeline_scripts, "token": conf.prefix + "_runall_all.tokens.txt"},
        output = {"dir": conf.target_dir,
                  "spot_peaks_combined": conf.hotspot_merge_both_passes_prefix + "_merge_all.hotspot.twopass.fdr0.01.merge.pks.bed",
                  "density_starch": conf.prefix + "_merge_all.tagdensity.bed.starch",
                  "spot": conf.prefix + "_merge_all.spot.out"},
        param = None))

def _macs2_on_reps(workflow, conf):
    """
    call peaks by MACS2, optionally
    """
    ## for all reads
    for target in conf.treatment_targets:
        ## keep all duplicate tags as hotspot
        macs2_on_rep = attach_back(workflow,
            ShellCommand(
                "{tool} callpeak -B -q 0.01 -f {param[format]} -g {param[species]} --keep-dup {param[keep_dup]} --shiftsize={param[shiftsize]} --nomodel -g {param[species]} \
                {param[treat_opt]} -n {param[description]}",
                tool="macs2",
                input={"treat": target + "_picard_sort.bam.all"},
                output={"peaks": target + "_macs2_all_peaks.bed",
                        "summit": target + "_macs2_all_summits.bed",
                        "treat_bdg": target + "_macs2_all_treat_pileup.bdg",
                        "ENCODE": target + "_macs2_all_peaks.narrowPeak",
                        "peaks_xls": target + "_macs2_all_peaks.xls",
                        "control_bdg": target + "_macs2_all_control_lambda.bdg"},
                param={"description": target + "_macs2_all", "keep_dup": "all", "shiftsize": 50, "species": conf.get("macs2", "species")},
                name="macs2_callpeak_rep"))
        macs2_on_rep.param["treat_opt"] = "-t " + macs2_on_rep.input["treat"]
        macs2_on_rep.update(param=conf.items("macs2"))
        if conf.seq_type == "se":
            macs2_on_rep.param["format"] = "BAM"
        elif conf.seq_type == "pe":
            macs2_on_rep.param["format"] = "BAMPE"

        ## For bedGraphToBigwiggle bugs, we need to remove coordinates outlier
        ## filter bdg file to remove over-border coordinates
        attach_back(workflow,
            ShellCommand(
                '{tool} intersect -a {input} -b {param[chrom_bed]} -f 1.00 > {output}',
                tool="bedtools",
                input=target + "_macs2_all_treat_pileup.bdg",
                output=target + "_macs2_all_treat_pileup.bdg.tmp",
                param={'chrom_bed': conf.get("lib", "chrom_bed")},
                name="bedGraph replicate filtering"))

        bdg2bw_treatrep = attach_back(workflow,
            ShellCommand(
                "{tool} {input} {param[chrom_len]} {output}",
                tool="bedGraphToBigWig",
                input=target + "_macs2_all_treat_pileup.bdg.tmp",
                output=target + "_treat_all.bw",
                param={"chrom_len": conf.get("lib", "chrom_len")}, name="bdg_to_bw"))

    ## for 5M reads
    for target in conf.treatment_targets:
        ## keep all duplicate tags as hotspot
        macs2_on_rep = attach_back(workflow,
            ShellCommand(
                "{tool} callpeak -B -q 0.01 -g {param[species]} --keep-dup {param[keep_dup]} --shiftsize={param[shiftsize]} --nomodel -g {param[species]} \
                {param[treat_opt]} -n {param[description]}",
                tool="macs2",
                input={"treat": target + "_picard_sort.bam.5M"},
                output={"peaks": target + "_5M_macs2_peaks.bed",
                        "summit": target + "_5M_macs2_summits.bed",
                        "treat_bdg": target + "_5M_macs2_treat_pileup.bdg",
                        "ENCODE": target + "_5M_macs2_peaks.narrowPeak",
                        "peaks_xls": target + "_5M_macs2_peaks.xls",
                        "control_bdg": target + "_5M_macs2_control_lambda.bdg"},
                param={"description": target + "_5M_macs2", "keep_dup": "all", "shiftsize": 50, "species": conf.get("macs2", "species")},
                name="macs2_callpeak_rep"))
        macs2_on_rep.param["treat_opt"] = "-t " + macs2_on_rep.input["treat"]
        macs2_on_rep.update(param=conf.items("macs2"))

        ## For bedGraphToBigwiggle bugs, we need to remove coordinates outlier
        ## filter bdg file to remove over-border coordinates
        attach_back(workflow,
            ShellCommand(
                '{tool} intersect -a {input} -b {param[chrom_bed]} -f 1.00 > {output}',
                tool="bedtools",
                input=target + "_5M_macs2_treat_pileup.bdg",
                output=target + "_5M_macs2_treat_pileup.bdg.tmp",
                param={'chrom_bed': conf.get("lib", "chrom_bed")},
                name="bedGraph replicate filtering"))

        bdg2bw_treatrep = attach_back(workflow,
            ShellCommand(
                "{tool} {input} {param[chrom_len]} {output}",
                tool="bedGraphToBigWig",
                input=target + "_5M_macs2_treat_pileup.bdg.tmp",
                output=target + "_5M_macs2_treat.bw",
                param={"chrom_len": conf.get("lib", "chrom_len")}, name="bdg_to_bw"))

def _macs2_on_combo(workflow, conf):
    # merge all treatments into one
    merge_bams_treat = ShellCommand(
        "{tool} merge {output[merged]} {param[bams]}",
        tool="samtools",
        input=[ target + "_picard_sort.bam.all" for target in conf.treatment_targets],
        output={"merged": conf.prefix + "_treatment.bam"})
    merge_bams_treat.param = {"bams": " ".join(merge_bams_treat.input)}
    attach_back(workflow, merge_bams_treat)

    macs2_on_merged = attach_back(workflow, ShellCommand(
        "{tool} callpeak -B -q 0.01 -f {param[format]} --keep-dup {param[keep_dup]} --shiftsize={param[shiftsize]} --nomodel -g {param[species]} \
        {param[treat_opt]} -n {param[description]}",
        tool="macs2",
        input={"merged": conf.prefix + "_treatment.bam"},
        output={"peaks": conf.prefix + "_peaks.bed",
                "summit": conf.prefix + "_summits.bed",
                "treat_bdg": conf.prefix + "_treat_pileup.bdg",
                "ENCODE": conf.prefix + "_peaks.narrowPeak",
                "peaks_xls": conf.prefix + "_peaks.xls",
                "control_bdg": conf.prefix + "_control_lambda.bdg"},
        param={"description": conf.prefix,
               "keep_dup": "all",
               "shiftsize": 50,
               "species": conf.get("macs2",  "species")},
        name="macs2_callpeak_merged"))

    macs2_on_merged.param["treat_opt"] = " -t " + macs2_on_merged.input["merged"]
    macs2_on_merged.update(param=conf.items("macs2"))

    if conf.seq_type == "se":
        macs2_on_merged.param["format"] = "BAM"
    elif conf.seq_type == "pe":
        macs2_on_merged.param["format"] = "BAMPE"

    ## BedClip
    ## prototype used here to do the similar thing on bedclip
    bed_clip = attach_back(workflow,
        ShellCommand(
            template="{tool} {input} {param[chrom_len]} {output}",
            tool="bedClip",
            input=conf.prefix + "_treat_pileup.bdg",
            output =conf.prefix + "_treat_pileup.bdg.tmp",
            param={'chrom_len': conf.get_path("lib", "chrom_len")},
            name="bedclip filter"))

    bdg2bw_treat = attach_back(workflow,
        ShellCommand(
            "{tool} {input[bdg]} {input[chrom_len]} {output[bw]}",
            tool="bedGraphToBigWig",
            input={"bdg": conf.prefix + "_treat_pileup.bdg.tmp",
                   "chrom_len": conf.get("lib", "chrom_len")},
            output={"bw": conf.prefix + "_macs2_treat_all.bw"},
            name="bdg_to_bw"))

def _peaks_calling_latex(workflow, conf):
    ## peaks number json
    if conf.peakcalltool == "hotspot":
        if len(conf.treatment_pairs) >= 2:

            attach_back(workflow, PythonCommand(
                    stat_peaks,
                    input = {"peaks": {"5M": [ conf.hotspot_reps_both_passes_5M_prefix[i] + ".hotspot.twopass.fdr0.01.merge.pks.bed" for i in range(len(conf.treatment_pairs)) ],
                                       "all": [ conf.hotspot_reps_both_passes_prefix[i] + ".hotspot.twopass.fdr0.01.merge.pks.bed" for i in range(len(conf.treatment_pairs)) ],
                                       "combo": conf.hotspot_merge_both_passes_prefix + "_merge_all.hotspot.twopass.fdr0.01.merge.pks.bed" }},
                    output = {"json": conf.json_prefix + "_peaks.json"},
                    param = {"tool": conf.peakcalltool}))
        else:
            attach_back(workflow, PythonCommand(
                    stat_peaks,
                    input = {"peaks": {"5M": [ conf.hotspot_reps_both_passes_5M_prefix[i] + ".hotspot.twopass.fdr0.01.merge.pks.bed" for i in range(len(conf.treatment_pairs)) ],
                                       "all": [ conf.hotspot_reps_both_passes_prefix[i] + ".hotspot.twopass.fdr0.01.merge.pks.bed" for i in range(len(conf.treatment_pairs)) ]}},
                    output = {"json": conf.json_prefix + "_peaks.json"},
                    param = {"tool": conf.peakcalltool}))

    elif conf.peakcalltool == "macs2":
        if len(conf.treatment_pairs) >= 2:
            attach_back(workflow, PythonCommand(
                    stat_peaks,
                    input = {"peaks": {"all": [ target + "_macs2_all_peaks.bed" for target in conf.treatment_targets ],
                                       "5M": [ target + "_5M_macs2_peaks.bed" for target in conf.treatment_targets ],
                                       "combo": conf.prefix + "_peaks.bed"}},
                    output = {"json": conf.json_prefix + "_peaks.json"},
                    param = {"tool": conf.peakcalltool}))
        else:
            attach_back(workflow, PythonCommand(
                    stat_peaks,
                    input = {"peaks": {"all": [ target + "_macs2_all_peaks.bed" for target in conf.treatment_targets ],
                                       "5M": [ target + "_5M_macs2_peaks.bed" for target in conf.treatment_targets ]}},
                    output = {"json": conf.json_prefix + "_peaks.json"},
                    param = {"tool": conf.peakcalltool}))

    ## peaks number latex
    attach_back(workflow, PythonCommand(
        peaks_doc,
        input = {"tex": tex, "json": conf.json_prefix + "_peaks.json"},
        output = {"latex": conf.latex_prefix + "_peaks.tex"},
        param = {"reps": len(conf.treatment_pairs)},
        name = "peaks number json"))

    ## 5M reads, calculate the merged two passes and peaks regions number
    ## 5M reads for macs2 optionally
    attach_back(workflow, PythonCommand(
        stat_spot_on_replicates,
        input = {"spot_files": [ target + "_5M.spot.out" for target in conf.treatment_targets ]},
        output = {"json": conf.json_prefix + "_sample_spot_5M.json"},
        name = "json spot",
        param = {"samples": conf.treatment_bases}))

    attach_back(workflow, PythonCommand(
        spot_doc,
        input = {"json": conf.json_prefix + "_sample_spot_5M.json",
                 "tex": tex},
        output = {"latex": conf.latex_prefix + "_spot.tex"},
        name = "doc spot 5M",
        param = {"samples": conf.treatment_bases, "reps": len(conf.treatment_pairs)}))

def _promoter_overlap(workflow, conf):
    """ use bedtools to get promotor peaks enrichment,
    Genome Promotor: cutoff 50% promotor overlaps with mappability regions bases number
    Peaks Promotor: cutoff 50% promotor overlaps with peaks regions bases number
    use 5M reads peak calling results
    --fraction-{ref,map,both,either}
    """
    ## genome promotor evaluation, use hotspot mappability files, bases
#    attach_back(workflow,
#        ShellCommand(
#            "{tool}  --fraction-ref {param[percentage]} --bases {input[ref]} {input[map]} > {output[count]}",
#            tool = "bedmap",
#            input = {"ref":conf.get("lib", "tss"), "map": conf.get("hotspot", "mappable_region")},
#            output = {"count": conf.prefix + "_promotor_bases.count"},
#            param = {"percentage": "0.5"}))

    ## use hotspot mappable_region for both hotspot and macs2 peak calling
    attach_back(workflow,
        ShellCommand(
            "{tool} -e -{param[percentage]} {input[ref]} {input[map]} | wc -l > {output[count]}",
            tool = "bedops",
            input = {"ref":conf.get("lib", "tss"), "map": conf.get("hotspot", "mappable_region")},
            output = {"count": conf.prefix + "_promotor.count"},
            param = {"percentage": "50%"}))

    ## peaks region evaluation
    for i, target in enumerate(conf.treatment_targets):
        if conf.peakcalltool == "hotspot":
            peaks = target + "_5M_velcro_non_overlap_peaks.bed.final"
        elif conf.peakcalltool == "macs2":
            peaks = target + "_5M_macs2_velcro_non_overlap_peaks.bed.final"
        attach_back(workflow,
            ShellCommand(
                "{tool} -e -{param[percentage]} {input[ref]} {input[promotor]} | wc -l > {output[count]} && wc -l {input[ref]} > {output[peaks]}",
                tool = "bedops",
                input = {"ref": peaks,
                         "promotor": conf.get("lib", "tss")},
                output = {"count": target + "_5M_promotor_overlap.count",
                          "peaks": target + "_peaks.count"},
                param = {"percentage": "50%"},
                name = "promotor overlaps"))

#        attach_back(workflow,
#            ShellCommand(
#                "{tool}  --fraction-ref {param[percentage]} --bases {input[ref]} {input[promotor]} > {output[count]}",
#                tool = "bedmap",
#                input = {"ref": target + "_5M_velcro_non_overlap_peaks.bed.final",
#                         "promotor": conf.get("lib", "tss")},
#                output = {"count": target + "_5M_promotor_bases.count"},
#                param = {"percentage": "50%"}))

    ## promotor json
    attach_back(workflow, PythonCommand(
        stat_promotor,
        input = {"peaks_promotor": [ target + "_5M_promotor_overlap.count" for target in conf.treatment_targets ],
                 "peaks": [ target + "_peaks.count" for target in conf.treatment_targets ],
                 "promotor": conf.prefix + "_promotor.count",
                 "mappable": conf.get("hotspot", "mappable_region")},
        output = {"json": conf.json_prefix + "_promotor.json"},
        param = {"samples": conf.treatment_bases}
    ))

    ## promotor latex document
    attach_back(workflow, PythonCommand(
        promotor_doc,
        input = {"json": conf.json_prefix + "_promotor.json",
                 "tex": tex},
        output = {"latex": conf.latex_prefix + "_promotor.tex"},
        param = {"samples": conf.treatment_bases,
                 "reps": len(conf.treatment_pairs)}))

def _conservation(workflow, conf):
    ## use 5M reads for evaluation
    for target in conf.treatment_targets:
        ## get non-promotor peaks regions
        if conf.peakcalltool == "hotspot":
            peaks = target + "_5M_velcro_non_overlap_peaks.bed.final"
            non_promotor = target + "_5M_non-promotor_peaks_bed"
            phascon = target + "_100bp_phastcon.score"
        elif conf.peakcalltool == "macs2":
            peaks = target + "_5M_macs2_velcro_non_overlap_peaks.bed.final"
            non_promotor = target + "_macs2_5M_non-promotor_peaks_bed"
            phascon = target + "_macs2_100bp_phastcon.score"
        attach_back(workflow,
            ShellCommand(
                "{tool} -v -a {input[peaks]} -b {input[tss]} > {output}",
                tool = "intersectBed",
                input = {"peaks": peaks, "tss": conf.get("lib", "tss")},
                output = non_promotor,
                name = "Get non-promotor peaks regions",
                param = None))

        get_top_peaks = attach_back(workflow,
            ShellCommand(
                "{tool} -r -g -k 5 {input} | head -n {param[peaks]} > {output}",
                tool="sort",
                input=non_promotor,
                output=non_promotor + ".top1000",
                param={'peaks': 1000}, name="top summits for conservation"))
        get_top_peaks.update(param=conf.items('conservation'))

        ## get summits is implemented in conservation_average.py
        attach_back(workflow,
            ShellCommand(
                "{tool} -w {param[width]} -d {input[phastcon_db]} {input[bed]} 1>{output}",
                tool = "conservation_average.py",
                input= {"phastcon_db": conf.get_path("lib", "phast"),
                        "bed": non_promotor + ".top1000"},
                output = phascon,
                param = {"width": 100}))

    ## Phastcon json
    attach_back(workflow,
        PythonCommand(stat_conserv,
            input = {"phastcon":[ target + "_100bp_phastcon.score" if conf.peakcalltool == "hotspot" else target + "_macs2_100bp_phastcon.score"
                      for target in conf.treatment_targets ]},
            output = {"json": conf.json_prefix + "_conserv.json"},
            param = {"sample": conf.treatment_bases}))

    ## Phaston latex
    attach_back(workflow,
        PythonCommand(conserv_doc,
            input = {"tex": tex, "json": conf.json_prefix + "_conserv.json"},
            output = {"latex": conf.latex_prefix + "_conserv.tex"},
            param = {"reps": len(conf.treatment_pairs),
                     "sample": conf.treatment_bases}))

def _union_DHS_overlap(workflow, conf):
    for target in conf.treatment_targets:
        if conf.peakcalltool == "hotspot":
            peaks = target + "_5M_velcro_non_overlap_peaks.bed.final"
            output = target + "_DHS_overlap_peaks_bed"
        elif conf.peakcalltool == "macs2":
            peaks = target + "_5M_macs2_velcro_non_overlap_peaks.bed.final"
            output = target + "_macs2_DHS_overlap_peaks_bed"

        attach_back(workflow,
            ShellCommand(
                "{tool} -wa -u  \
                -a {input[pks_spot_bed]} -b {input[DHS_peaks_bed]} > {output}",
                tool="intersectBed",
                input={"pks_spot_bed": peaks,
                       "DHS_peaks_bed": conf.get("lib", "dhs")},
                output=output,
                name = "Write out DHS overlap BED",
                param = None))

    attach_back(workflow, PythonCommand(
        stat_dhs,
        input={"dhs_peaks": [ target + "_DHS_overlap_peaks_bed" if conf.peakcalltool == "hotspot" else target + "_macs2_DHS_overlap_peaks_bed"
                            for target in conf.treatment_targets ],
               "pks_spot_bed": [ target + "_5M_velcro_non_overlap_peaks.bed.final" if conf.peakcalltool == "hotspot" else target + "_5M_macs2_velcro_non_overlap_peaks.bed.final"
                                for target in conf.treatment_targets ]},
        output = {"json": conf.json_prefix + "_dhs.json"},
        param= {"samples":conf.treatment_bases},
        name="DHS summary"))

    attach_back(workflow, PythonCommand(
        DHS_doc,
        input = {"tex": tex, "json": conf.json_prefix + "_dhs.json"},
        output = {"latex": conf.latex_prefix + "_dhs.tex"},
        param = {"reps": len(conf.treatment_pairs), "samples": conf.treatment_bases}))

def report(workflow, conf):
    collector = []

    ## begin doc and table header
    attach_back(workflow, PythonCommand(
        begin_doc,
        input = tex,
        output = {"begin": conf.latex_prefix + "_begin.tex",
                  "header": conf.latex_prefix + "_header.tex"},
        param = {"rep": len(conf.treatment_pairs), "id": underline_to_space(conf.id)}))

    collector.append(conf.latex_prefix + "_begin.tex")
    collector.append(conf.latex_prefix + "_header.tex")

    # end doc
    attach_back(workflow, PythonCommand(
        end_doc,
        input = {"tex": tex},
        output = {"table_end": conf.latex_prefix + "_table_end.tex",
                  "doc_end": conf.latex_prefix + "_doc_end.tex"},
        param = {}))
    collector.append(conf.latex_prefix + "raw.tex")
    collector.append(conf.latex_prefix + "len.tex")
    collector.append(conf.latex_prefix + "seq_quality.tex")
    collector.append(conf.latex_prefix + "mapping.tex")
    collector.append(conf.latex_prefix + "_frag.tex")
    collector.append(conf.latex_prefix + "redun.tex")
    collector.append(conf.latex_prefix + "_peaks.tex")
    collector.append(conf.latex_prefix + "_spot.tex")
    if len(conf.treatment_pairs) >= 2:
        collector.append(conf.latex_prefix + "_reps.tex")
    collector.append(conf.latex_prefix + "_promotor.tex")
    collector.append(conf.latex_prefix + "_conserv.tex")
    collector.append(conf.latex_prefix + "_dhs.tex")
    collector.append(conf.latex_prefix + "_table_end.tex")
    collector.append(conf.latex_prefix + "_doc_end.tex")
    print(collector)

    integrate_doc = attach_back(workflow, ShellCommand(
            "cat {param[input]} > {output[tex]} && {tool} -output-directory {output[dir]} -jobname={param[name]} {output[tex]}\
             && {tool} -output-directory {output[dir]} -jobname={param[name]} {output[tex]}",
            tool="pdflatex",
            input= collector,
            # output[pdf] should use "conf.prefix" to have the absolute path
            output={"tex": conf.prefix + "_report.tex", "dir": conf.target_dir, "pdf": conf.prefix + "_report.pdf"},
            # param[name] should use "conf.id" to avoid using absolute path
            param={"name": conf.id + "_report"},
            name="report"))

    integrate_doc.param.update({"input": " ".join(integrate_doc.input)})

def prepare_clean_up(workflow, conf):
    """
    package all the necessary results and delete temporary files
    preserve bed, bigwiggle, starch, bam and pdf report
    """
    p_list = ['*.bam', '*.xls', '*_summits.bed', '*_peaks.bed', '*.bw',
              '*.R', '*.zip', '*cor*', 'json', "*summary*",
              "*seqpos", "*fastqc", '*latex', "*.conf"]

    p_pattern = [os.path.join(conf.target_dir, p) for p in p_list]

    final_dir = conf.target_dir + '/dataset_' + conf.id
    attach_back(workflow,
        ShellCommand("if [ ! -d '{output}' ]; then mkdir -p {output}; fi",
            output=final_dir))

    for pf in p_pattern:
        if not glob(pf):
            print(pf)
            continue
        move = attach_back(workflow,
            ShellCommand('mv {param[preserve_files]} {output[dir]} \n# Pattern: {param[p_pattern]}',
                output={"dir": final_dir},
                param={"preserve_files": " ".join(glob(pf)),
                       "p_pattern": pf}, ))
        move.allow_fail = True

def prepare_purge(workflow, conf):
    d_list = ["json", "latex", conf.id + "_whole.*"]
    d_pattern = [os.path.join(conf.target_dir, d) for d in d_list]

    for df in d_pattern:
        if not glob(df):
            print(df)
            continue
        deleted = attach_back(workflow,
            ShellCommand("rm -r {param[deleted_files]}",
                param={"deleted_files": " ".join(glob(df))}))
        deleted.allow_fail = True

def create_workflow(args, conf):
    workflow = Workflow(name="Main")
    bld = GCAPBuilder(workflow, conf)

    if args.sub_command == "clean":
        ## not implemented yet
        bld.build(prepare_clean_up)
        return workflow

    if args.sub_command == "purge":
        bld.build(prepare_purge)
        return workflow

    if args.skip_step:
        skipped_steps = [int(i) for i in args.skip_step.split(",")]
    else:
        skipped_steps = []

    step_checker = StepChecker(args.start_step, args.end_step, skipped_steps)

    have_treat_reps = len(conf.treatment_pairs) >= 2 ## replicates
    has_dhs = conf.get("lib", "dhs")
    need_run = step_checker.need_run

    bld.attach_back(ShellCommand(
        "if [ ! -d '{output}' ]; then mkdir -p {output}; fi",
        output=conf.target_dir))

    ## TODO: replace this by Jim's codes
    if need_run(1):
        bld.build(_fastqc)

    ## TODO: add bwa
    if need_run(2):
        bld.build(_lib_contamination)

    if need_run(3):
        bld.build(_reads_mapping) ## mapping and convert format to sorted bam

    if need_run(4):
        bld.build(_library_complexity)

    if need_run(5):
        bld.build(_fragment_size)

    if need_run(6):
        if conf.peakcalltool == "hotspot":
            bld.build(_hotspot_on_replicates)
        elif conf.peakcalltool == "macs2":
            bld.build(_macs2_on_reps)

        bld.build(_peaks_reps_preprocess)

        if have_treat_reps:
            bld.build(_peaks_reps_evaluating)
            if conf.peakcalltool == "hotspot":
                bld.build(_hotspot_combo)
            elif conf.peakcalltool == "macs2":
                bld.build(_macs2_on_combo)

        bld.build(_peaks_calling_latex)

    ## need modify here
    if need_run(7):
        bld.build(_promoter_overlap)

    if need_run(8):
        bld.build(_conservation)

    if has_dhs and need_run(9):
        bld.build(_union_DHS_overlap)

    if need_run(10):
        bld.build(report)
    return workflow

def main(args=None):
    args = parse_args(args)
    print("Arguments:", args)

    if args.sub_command in ["run", "clean", "purge"]:
        conf = Conf(args.config)
        workflow = create_workflow(args, conf)

        workflow.set_option(
            verbose_level=args.verbose_level,
            dry_run_mode=args.dry_run,
            resume=args.resume,
            allow_dangling=args.allow_dangling)

        workflow.invoke()

    elif args.sub_command == "batch":
        with open(args.batch_config) as batch_file:
            for a_conf in batch_file:
                a_conf = a_conf.strip()
                conf = Conf(a_conf)
                workflow = create_workflow(args, conf)
                workflow.set_option(
                    verbose_level=args.verbose_level,
                    dry_run_mode=args.dry_run,
                    resume=args.resume,
                    allow_dangling=args.allow_dangling)
                workflow.invoke()

if __name__ == "__main__":
    main()

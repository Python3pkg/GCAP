#!/usr/bin/env python3

import os
import re
from glob import glob

import argparse
from samflow.command import ShellCommand, PythonCommand
from samflow.workflow import Workflow, attach_back
from pkg_resources import resource_filename

from gcap.config import Conf
from gcap.helpers import *

## hotspot
token_file = resource_filename("gcap", "static/runall.tokens.txt")
pipeline_scripts = resource_filename("gcap", "pipeline-scripts")

## latex template
tex = resource_filename("gcap", "static/gcap_template.tex")

def parse_args(args=None):
    """
    If args is None, argparse will parse from sys.argv
    """
    description = "GCAP :  Global Chromatin Accessibility Pipeline"
    parser = argparse.ArgumentParser(description=description)
    sub_parsers = parser.add_subparsers(help="sub-command help", dest="sub_command")

    parser_run = sub_parsers.add_parser("run", help="run pipeline using a config file",
        description="GCAP-run: Run GCAP pipeline using a config file")

    parser_batch = sub_parsers.add_parser("batch", help="run pipeline for multiple datasets")
    parser_batch.add_argument("-b", "--batch-config", dest="batch_config", required=True, help="batch file")
    parser_purge = sub_parsers.add_parser("purge")
    parser_clean = sub_parsers.add_parser("clean", help="Move result file into a new folder and delete other files",
        description="GCAP-run: Run GCAP pipeline using a config file")

    for p in (parser_run, parser_batch):
        p.add_argument("--from", dest="start_step", default=0, type=int,
            help="Only step after this number will be processed")
        p.add_argument("--to", dest="end_step", default=100, type=int,
            help="Only step before this number will be processed ")
        p.add_argument("--skip", dest="skip_step", default="",
            help="Steps to skip, use comma as seperator")

    for p in (parser_run, parser_batch, parser_purge, parser_clean):
        p.add_argument("-v", "--verbose-level", dest="verbose_level", type=int, default=2)
        p.add_argument("--dry-run", dest="dry_run", action="store_true", default=False)
        p.add_argument("--allow-dangling", dest="allow_dangling", action="store_true", default=False)
        p.add_argument("--resume", dest="resume", action="store_true", default=False)
        p.add_argument("--remove", dest="clean", action="store_true", default=False)

    for p in (parser_run, parser_purge, parser_clean):
        p.add_argument("-c", "--config", dest="config", required=True,
            help="specify the config file to use", )

    return parser.parse_args(args)

def _versatile_format(workflow, conf):
    """
    support SAM/BAM and BED reads files
    """
    for n, target in enumerate(conf.treatment_targets):
            if conf.seq_type.startswith("bam"):
                attach_back(workflow,
                    ShellCommand(
                        "{tool} view -h {input[bam]} > {output[sam]}",
                        tool = "samtools",
                        input = {"bam": conf.treatment_bam[n]},
                        output = {"sam": target + "_all.sam"},
                        name = "samtools conversion"))
            elif conf.seq_type.startswith("sam"):
                attach_back(workflow,
                    ShellCommand(
                        "{tool} -sf {input[sam]} {output[sam]}",
                        tool = "ln",
                        input = {"sam": conf.treatment_bam[n]}, ## actually sam files
                        output = {"sam": target + "_all.sam"}))
            elif conf.seq_type.startswith("bed"):
                attach_back(workflow,
                    ShellCommand(
                        "{tool} -sf {input[bed]} {output[bed]}",
                        tool = "ln",
                        input = {"bed":conf.treatment_bed[n]}, ## actually bed files
                        output = {"bed":target + "_all.bed"}))

def _sample_reads(workflow, conf, N, format):
    """
    get top 100k or 5M reads for following analysis
    """
    if format == "fastq":
        if conf.seq_type == "se":
            for n, target in enumerate(conf.treatment_targets):
#                attach_back(workflow,
#                    PythonCommand(single_end_fastq_sampling,
#                        input={"fastq": conf.treatment_raws[n]},
#                        output={"fastq_sample": target + "_%s.fq" % suffix},
#                        param={"random_number": N}))     ## use 100kb random reads
                attach_back(workflow,
                    ShellCommand(
                        "{tool} -n {param[top]} {input[fastq]} > {output[fastq_sample]}",
                        tool = "head",
                        input = {"fastq": conf.treatment_raws[n]},
                        param = {"top": N*4},
                        output = {"fastq_sample": target + "_100k.fastq"}))
        elif conf.seq_type == "pe":
            for raw, target in conf.treatment_pairs_pe:
#                attach_back(workflow,
#                    PythonCommand(pair_end_fastq_sampling,
#                    input={"fastq": raw},
#                    output={"fastq_sample": [ i + "_%s.fq" % suffix for i in target ]},
#                    param={"random_number": N}))     ## use 100kb random reads
                attach_back(workflow,
                    ShellCommand(
                        "{tool} -n {param[top]} {input[fastq][0]} > {output[fastq_sample][0]} && \
                        {tool} -n {param[top]} {input[fastq][1]} > {output[fastq_sample][1]} ",
                        tool = "head",
                        input = {"fastq": raw},
                        param = {"top": N*4},
                        output = {"fastq_sample": [ i + "_100k.fastq" for i in target ]}))

    elif format == "sam":
        suffix = "100k" if N == 100000 else "5M"
        for target, s in zip(conf.treatment_targets, conf.treatment_bases):
            ## built-in sampling for raw reads
            if conf.seq_type.startswith("bam") or conf.seq_type.startswith("sam"):
                attach_back(workflow, PythonCommand(
                    sampling,
                    input = {"sam": target + "_all.sam"},
                    output = {"sam_sample": target + "_%s.sam" % suffix},
                    param = {"random_number": N}))
            else:
                attach_back(workflow, PythonCommand(
                    sampling,
                    input = {"sam": target + "_all.sam"},
                    output = {"sam_sample": target + "_%s.sam" % suffix},
                    param = {"random_number": N,
                             "map_or_unmap": "both",
                             "se_or_pe": conf.seq_type}))

    elif format == "bed":
        for target in conf.treatment_targets:
            ## macs2 randsample sampling mappable reads to BED reads files
            if "sample" in dict(conf.items("macs2")):
                attach_back(workflow, ShellCommand(
                    "{tool} randsample -t {input[bed]} -n {param[rand_number]} -o {output[bed]}",
                    tool = "macs2",
                    input = {"bed": target + "_all.bed"},
                    output = {"bed": target + "_5M.bed"}, ## output bed file, to be uniform with other sampling methods
                    param = {"rand_number": N}))
            else:
                attach_back(workflow, PythonCommand(
                    sampling,                ## take all bed files as SE data
                    input = {"sam": target + "_all.bed"},
                    output = {"sam_sample": target + "_5M.bed"},
                    param = {"random_number": N}))

def _fastqc(workflow, conf):
    """
    1a. use top 100k reads instead of sampled reads for fastqc evaluation
    at the same time, prepare input files
    """
    if conf.seq_type == "pe": ## PE
        _sample_reads(workflow, conf, 100000, "fastq")
        for n, target in enumerate(conf.treatment_pair_data):
            for p in target:
                fastqc_run = attach_back(workflow,
                    ShellCommand(
                        "{tool} {input[fastq_sample]} --extract -t {param[threads]} -o {output[target_dir]}",
                        input= {"fastq_sample": p + "_100k.fastq"},
                        output={"target_dir": conf.target_dir,
                                "fastqc_summary": p + "_100k_fastqc/fastqc_data.txt"},
                        tool="fastqc",
                        param={"threads": 4}))
                fastqc_run.update(param=conf.items("fastqc"))
            attach_back(workflow, PythonCommand(stat_fastqc,
                input = {"fastqc_summaries": [ p + "_100k_fastqc/fastqc_data.txt" for p in target ]},
                output = {"json": conf.json_prefix + "_rep" + str(n+1) + "_fastqc.json"},
                param = {"samples": target }))
        attach_back(workflow, PythonCommand(
            fastqc_doc,
            input = {"tex": tex, "json": [ conf.json_prefix + "_rep" + str(n+1) + "_fastqc.json" for n in range(len(conf.treatment_pair_data)) ]},
            output = {"seq": conf.latex_prefix + "seq_quality.tex", "len": conf.latex_prefix + "len.tex"},
            param = {"seq_type": conf.seq_type, "reps": len(conf.treatment_pairs),
                     "pe_samples": [ target for target in conf.treatment_pair_data ]}))

    elif conf.seq_type == "se":
        _sample_reads(workflow, conf, 100000, "fastq")
        for target in conf.treatment_targets:
            fastqc_run = attach_back(workflow,
                ShellCommand(
                    "{tool} {input[fastq_sample]} --extract -t {param[threads]} -o {output[target_dir]}",
                    input= {"fastq_sample": target +  "_100k.fastq"},
                    output={"target_dir": conf.target_dir,
                            "fastqc_summary": target + "_100k_fastqc/fastqc_data.txt"},
                    tool="fastqc",
                    param={"threads": 4},
                    name = "fastqc"))
            fastqc_run.update(param=conf.items("fastqc"))
        attach_back(workflow, PythonCommand(stat_fastqc,
            input = {"fastqc_summaries": [ t + "_100k_fastqc/fastqc_data.txt" for t in conf.treatment_targets ]},
            output = {"json": conf.json_prefix + "_fastqc.json"},
            param = {"samples": conf.treatment_bases}))
        attach_back(workflow, PythonCommand(
            fastqc_doc,
            input = {"tex": tex, "json": conf.json_prefix + "_fastqc.json"},
            output = {"seq": conf.latex_prefix + "seq_quality.tex", "len": conf.latex_prefix + "len.tex"},
            param = {"seq_type": conf.seq_type, "reps": len(conf.treatment_pairs), "se_samples": conf.treatment_bases}))

    elif conf.seq_type.startswith("bam") or conf.seq_type.startswith("sam"):
        _versatile_format(workflow, conf)
        _sample_reads(workflow, conf, 100000, "sam")
        for target in conf.treatment_targets:
            attach_back(workflow,
                ShellCommand(
                    "{tool} view -bt {input[chrom_len]} {input[sam]} -o {param[tmp_bam]} && \
                    {tool} sort -m {param[max_mem]} {param[tmp_bam]} {param[output_prefix]}",
                    tool="samtools",
                    input={"sam": target + "_100k.sam", "chrom_len": conf.get_path("lib", "chrom_len")},
                    output={"bam": target + "_100k.bam"},
                    param={"tmp_bam": target + ".tmp.bam", "output_prefix": target + "_100k",
                           "max_mem": 5000000000},
                    name = "sampled sam to bam"
                )) # Use 5G memory as default
        file_suffix = "_100k.bam"
        suffix = "_100k" ## output suffix
        ## for single end 100k fastq or PE, SE 100k sam
        for target in conf.treatment_targets:
                fastqc_run = attach_back(workflow,
                    ShellCommand(
                        "{tool} {input[fastq_sample]} --extract -t {param[threads]} -o {output[target_dir]}",
                        input= {"fastq_sample": target +  file_suffix},
                        output={"target_dir": conf.target_dir,
                                "fastqc_summary": target + "%s_fastqc/fastqc_data.txt" % suffix},
                        tool="fastqc",
                        param={"threads": 4},
                        name = "fastqc"))
                fastqc_run.update(param=conf.items("fastqc"))
        ## get per sequence quality median
        attach_back(workflow, PythonCommand(stat_fastqc,
                input = {"fastqc_summaries": [ t + "%s_fastqc/fastqc_data.txt" % suffix for t in conf.treatment_targets ]},
                output = {"json": conf.json_prefix + "_fastqc.json"},
                param = {"samples": conf.treatment_bases}))
        ## sequence quality latex load, reads length
        attach_back(workflow, PythonCommand(
            fastqc_doc,
            input = {"tex": tex, "json": conf.json_prefix + "_fastqc.json"},
            output = {"seq": conf.latex_prefix + "seq_quality.tex", "len": conf.latex_prefix + "len.tex"},
            param = {"seq_type": conf.seq_type, "reps": len(conf.treatment_pairs), "se_samples": conf.treatment_bases})) ## single bam samples

    elif conf.seq_type.startswith("bed"): ## not evaluate sequence quality
        _versatile_format(workflow, conf)

def _lib_contamination(workflow, conf):
    """ input PE and SE sampled data 100 K to run,
     output json for library contamination, if not correct, shutdown
    """
    ## bowtie mapping back to mouse, rat and human
    if conf.seq_type == "se":
        for target in conf.treatment_targets:
            for species in dict(conf.items("contaminate")):
                attach_back(workflow,
                    ShellCommand(
                        "{tool} -p {param[threads]} -S -m {param[max_align]} \
                        {param[genome_index]} {input[fastq_sample]} {output[sam]} 2> {output[bowtie_summary]}",
                        input={"fastq_sample": target + "_100k.fastq"},
                        output={"sam": target + "_100k.sam",
                                "bowtie_summary": target + species + "_contam_summary.txt"},
                        tool="bowtie",
                        param={"threads": 4,
                               "max_align": 1,
                               "genome_index": conf.get_path("contaminate", species)}))
        all_species = [i for i, _ in conf.items("contaminate")]
        bowtie_summaries = []
        for target in conf.treatment_targets:
            bowtie_summaries.append([target + species + "_contam_summary.txt" for species in all_species])

        ## if library is contaminated, breaks
        attach_back(workflow,
            PythonCommand(stat_contamination,
                input={"bowtie_summaries": bowtie_summaries},
                output={"json": conf.json_prefix + "_contam.json"},
                param={"samples": conf.treatment_bases,
                       "id": conf.id,
                       "species": all_species,
                       "correct_species": conf.get("Basis", "species")}))

    elif conf.seq_type == "pe":
        for n, target in enumerate(conf.treatment_pair_data):
            for species in dict(conf.items("contaminate")):
                attach_back(workflow,
                    ShellCommand(
                        "{tool} -X 600 --chunkmbs 300 -n 1 -p {param[threads]} -S -m {param[max_align]} \
                        {param[genome_index]} -1 {input[fastq_sample][0]} -2 {input[fastq_sample][1]} {output[sam]} 2> {output[bowtie_summary]}",
                        input={"fastq_sample": [ t + "_100k.fastq" for t in target ]},
                        output={"sam": conf.treatment_targets[n] + "_100k.sam",
                                "bowtie_summary": conf.treatment_targets[n] + species + "_contam_summary.txt"},
                        tool="bowtie",
                        param={"threads": 4,
                               "max_align": 1,
                               "genome_index": conf.get_path("contaminate", species)},
                        name = "library contamination"))
        all_species = [i for i, _ in conf.items("contaminate")]
        bowtie_summaries = []
        for target in conf.treatment_targets:
            bowtie_summaries.append([target + species + "_contam_summary.txt" for species in all_species])

        ## if library is contaminated, breaks
        attach_back(workflow,
            PythonCommand(stat_contamination,
                input={"bowtie_summaries": bowtie_summaries},
                output={"json": conf.json_prefix + "_contam.json"},
                param={"samples": conf.treatment_bases,
                       "id": conf.id,
                       "species": all_species,
                       "correct_species": conf.get("Basis", "species")}))

    elif conf.seq_type.startswith("bam") or conf.seq_type.startswith("sam") or conf.seq_type.startswith("bed"):
        pass

def _bowtie(workflow, conf):
    """
    Only all reads mapping, optionally
    """
    if conf.seq_type == "se":
        for raw, target in conf.treatment_pairs:
            bowtie = attach_back(workflow,
                            ShellCommand(
                                "{tool} -n 1 -p {param[threads]} -S -m {param[max_align]} \
                                {param[genome_index]} {input[fastq]} {output[sam]} 2> {output[bowtie_summary]}",
                                input={"fastq": raw},
                                output={"sam": target + "_all.sam",
                                        "bowtie_summary": target + "all_um_summary.txt"},
                                tool="bowtie",
                                param={"threads": 4,
                                       "max_align": 1,
                                       "genome_index": conf.get_path("lib", "genome_index")}))
            bowtie.update(param = conf.items("bowtie"))

    elif conf.seq_type == "pe":
        for n, target in enumerate(conf.treatment_targets):
            bowtie = attach_back(workflow,
                        ShellCommand(
                            "{tool} -X 600 --chunkmbs 300 -n 1 -p {param[threads]} -S -m {param[max_align]} \
                            {param[genome_index]} -1 {input[fastq][0]} -2 {input[fastq][1]} {output[sam]} 2> {output[bowtie_summary]}",
                            input={"fastq": conf.treatment_raws[n]},
                            output={"sam": target + "_all.sam",
                                    "bowtie_summary": target + "all_um_summary.txt"},
                            tool="bowtie",
                            param={"threads": 4,
                                   "max_align": 1,
                                   "genome_index": conf.get_path("lib", "genome_index")}))
            bowtie.update(param = conf.items("bowtie"))
    elif conf.seq_type.startswith("bam") or conf.seq_type.startswith("sam") or conf.seq_type.startswith("bed"): ## skip mapping
        pass

## default
def _bwa(workflow, conf):
    """bwa genome fasta genome index
      2 bwa aln genome.index.fa s_4_1_sequence.fastq > s_4_1.sai
      3 bwa aln genome.index.fa s_4_2_sequence.fastq > s_4_2.sai
      4 bwa sampe genome.index.fa s_4_1.sai s_4_2.sai s_4_1_sequence.fastq s_4_2_sequence.fastq > s_4.sam
    """
    if conf.seq_type == "pe":
        for n, target in enumerate(conf.treatment_targets):
            for pair in conf.treatment_raws[n]:
                attach_back(workflow, ShellCommand(
                    "{tool} aln -t {param[threads]} {input[index]} {input[fastq]} > {output[sai]}",
                    tool = "bwa",
                    input = {"index": conf.get("lib", "genome_index"),
                             "fastq": pair},
                    output = {"sai": pair + ".sai"},
                    param = {"threads": 4}))
            attach_back(workflow, ShellCommand(
                "{tool} sampe {input[index]} {input[sai][0]} {input[sai][1]} {input[fastq][0]} {input[fastq][1]} > {output[sam]}",
                tool = "bwa",
                input = {"index": conf.get("lib", "genome_index"),
                         "fastq": [ pair for pair in conf.treatment_raws[n] ],
                         "sai": [ pair + ".sai" for pair in conf.treatment_raws[n] ]},
                output = {"sam": target + "_all.sam"}))

    elif conf.seq_type == "se":
        for raw, target in conf.treatment_pairs:
                attach_back(workflow, ShellCommand(
                    "{tool} aln {input[index]} {input[fastq]} > {output[sai]}",
                    tool = "bwa",
                    input = {"index": conf.get("lib", "genome_index"),
                             "fastq": raw},
                    output = {"sai": target + ".sai"}))
                attach_back(workflow, ShellCommand(
                    "{tool} samse {input[index]} {input[sai]} {input[fastq]} > {output[sam]}",
                    tool = "bwa",
                    input = {"index": conf.get("lib", "genome_index"),
                             "fastq": raw, "sai": target + ".sai"},
                    output = {"sam": target + "_all.sam"}))

    elif conf.seq_type.startswith("bam") or conf.seq_type.startswith("sam") or conf.seq_type.startswith("bed"): ## skip mapping
        pass

def _reads_mapping(workflow, conf):
    """
    reads mapping for all reads
    and convert sam to bam, sort
    Sam or Bam input, only do statistics
    """
    if conf.maptool == "bowtie":
        _bowtie(workflow, conf)
    elif conf.maptool == "bwa":
        _bwa(workflow, conf)

    ## filter mitochondria, chrX, chrY tags
    if not conf.seq_type.startswith("bed"):
        uniq_tag = resource_filename("gcap", "pipeline-scripts/uniq_map_exclude_chrM.sh")
        for target in conf.treatment_targets:
            ## TODO: bowtie and bwa specifically shell script
            attach_back(workflow, ShellCommand(
                "{tool} -f {param[script]} {input[sam]} > {output[count]}",
                tool = "awk",
                input = {"sam": target + "_all.sam"},
                output = {"count": target + "_uniq_tag_autosome.count"},
                param = {"script": uniq_tag}))
        attach_back(workflow, PythonCommand(
            autosome_map,
            input = {"count": [ target + "_uniq_tag_autosome.count" for target in conf.treatment_targets ]},
            output = {"json": conf.json_prefix + "_um_autosome.json"}, param = {"samples": conf.treatment_bases}))
        ## raw reads and reads mapping qc
        attach_back(workflow, PythonCommand(
            reads_doc,
            input = {"tex": tex, ## "json": conf.json_prefix + "_mapping.json", ## for uniform reads statistics
                     "json_autosome": conf.json_prefix + "_um_autosome.json"},
            output = {"raw": conf.latex_prefix + "raw.tex",
                      "mapping": conf.latex_prefix + "mapping.tex"},
            param = {"seq_type": conf.seq_type,
                     "reps": len(conf.treatment_pairs),
                     "samples": conf.treatment_bases}))

        ## convert to BAM file
        for n, target in enumerate(conf.treatment_targets):
            if not conf.seq_type.startswith("bam"):
                attach_back(workflow,
                    ShellCommand(
                        "{tool} view -bt {input[chrom_len]} {input[sam]} -o {output[bam]}",
                        tool="samtools",
                        input={"sam": target + "_all.sam", "chrom_len": conf.get_path("lib", "chrom_len")},
                        output={"bam": target + ".bam"}))
                workflow.update(param=conf.items("lib"))
            else:
                ## BAM input Files do not need to convert
                link = attach_back(workflow, ShellCommand(
                    "{tool} -sf {input[bam]} {output[bam]}",
                    tool = "ln",
                    input = {"bam": conf.treatment_bam[n]},
                    output = {"bam": target + ".bam"},   ## actually, may not be sorted bam files
                    param =None))
    else: ## bed files no reads statistics
        pass

def _library_complexity(workflow, conf):
    """  sampling 5M reads for following analysis
    use 5M raw reads for estimation
    """
    ## choose sampling modes
    if not conf.seq_type.startswith("bed"):
        ## top 5M raw reads
        _sample_reads(workflow, conf, 5000000, "sam")
        # picard markduplicates need SortSam
        ## for fastq files
        for target in conf.treatment_targets:
            lib_sort = attach_back(workflow, ShellCommand(
                "{tool} -Xmx5g -XX:ParallelGCThreads={param[threads]} -jar {param[sort]} I={input[bam]} O={output[bam]} SO=coordinate \
                VALIDATION_STRINGENCY=SILENT",
                tool = "java",
                input = {"bam": target + "_5M.sam"},
                output = {"bam": target + "_5M_sort.bam"},
                param = {"threads": 4},
                name = "Library Sort Sam to Bam"))
            lib_sort.update(param = conf.items("picard"))
        for target in conf.treatment_targets:
#            lib_dup = attach_back(workflow, ShellCommand(
#                "{tool} -Xmx5g -XX:ParallelGCThreads={param[threads]} -jar {param[markdup]} I={input[bam]} O={output[bam]} METRICS_FILE={output[metrics]} REMOVE_DUPLICATES=false \
#                VALIDATION_STRINGENCY=SILENT",
#                tool = "java",
#                input = {"bam": target + "_picard_sort.bam.5M"},
#                output = {"bam": target + "_markdup.bam.5M", "metrics": target + "_markdup_metric.5M"},
#                param = {"threads": 4}))
#            lib_dup.update(param = conf.items("picard"))
#        attach_back(workflow, PythonCommand(
#            stat_redun_picard,
#            input = {"picard": [target + "_markdup_metric.5M" for target in conf.treatment_targets ]},
#            output = {"json": conf.json_prefix + "_redun.json"}, param = {"samples": conf.treatment_bases, "format": "notbed"}))
            ## use census to estimation instead of picard
            attach_back(workflow, ShellCommand(
                "{tool} {param[hist]} {param[se_or_pe]} {param[exclude]} {input[sorted_bam]} | {tool} {param[calc]} - > {output[metrics]}",
                tool = "python2.7",
                input = {"sorted_bam": target + "_5M_sort.bam"},
                output = {"metrics": target + "_census.metric"},
                param = {"exclude": conf.get("census", "census_exclude"),
                         "hist": conf.get("census", "hist"),
                         "calc": conf.get("census", "calc"),
                         "se_or_pe": "-s" if "se" in conf.seq_type else " "}))
        attach_back(workflow, PythonCommand(
                stat_redun_census,
                input = {"census": [target + "_census.metric" for target in conf.treatment_targets]},
                output = {"json": conf.json_prefix + "_redun.json"},
                param = {"samples": conf.treatment_bases, "format": "notbed"}))
        attach_back(workflow, PythonCommand(
            redundancy_doc,
            input = {"tex": tex, "json": conf.json_prefix + "_redun.json"},
            output = {"redun": conf.latex_prefix + "redun.tex"},
            param = {"reps": len(conf.treatment_pairs), "samples": conf.treatment_bases}))

    else:  ## get top 5M reads, use shell to calculate redundancy simply
        _sample_reads(workflow, conf, 5000000, "bed")

def _strand_cor(workflow, conf):
    """ use ccQualityControl run_spp.R to estimate
    Rscript run_spp.R -c=<tagAlign/BAMfile> -savp -out=<outFile>
    support BAM/SAM, fastq input
    """
    if not conf.seq_type.startswith("bed"):
        for target in conf.treatment_targets:
            attach_back(workflow, ShellCommand(
                "{tool} {param[spp]} -c={input[bam]} -savp -out={output}",
                tool = "Rscript",
                input = {"bam": target + "_5M_sort.bam"},
                output = target + "_strand_cor",
                param = {"spp": conf.get("tool", "spp")}))
        ## add doc and json for NSC, RSC
        attach_back(workflow, PythonCommand(
            stat_strand_cor,
            input = {"metric": [target + "_strand_cor" for target in conf.treatment_targets]},
            output = {"json": conf.json_prefix + "_strand_cor.json"},
            param = {"samples": conf.treatment_bases}))
        attach_back(workflow, PythonCommand(
            strand_cor_doc,
            input = {"json": conf.json_prefix + "_strand_cor.json", "tex": tex},
            output = {"nsc_latex": conf.latex_prefix + "_nsc.tex", "rsc_latex": conf.latex_prefix + "_rsc.tex"},
            param = {"samples": conf.treatment_bases,
                     "reps": len(conf.treatment_pairs)}))

def _fragment_size(workflow, conf):
    """
    picard for PE,
    MACS2 for SE, because of background noise, we choose alternative fragment size closest to size selection
    """
    ## picard for pair end
    if conf.seq_type == "pe":
        for target in conf.treatment_targets:
            insert_size = attach_back(workflow, ShellCommand(
                "{tool} -Xmx5g -XX:ParallelGCThreads={param[threads]} -jar {param[insertsize]} HISTOGRAM_FILE={output[pdf]} I={input[bam]} O={output[insert]} VALIDATION_STRINGENCY=SILENT",
                tool = "java",
                input = {"bam": target + "_5M_sort.bam"},
                output = {"pdf": target + "_picard_insert_5M.pdf", "insert": target + "_insert_metric.5M"},
                param = {"threads": 4}))
            insert_size.update(param = conf.items("picard"))

        attach_back(workflow, PythonCommand(
            stat_frag_std,
            input = {"insert": [ target + "_insert_metric.5M" for target in conf.treatment_targets ]},
            output = {"json": conf.json_prefix + "_frag.json"},
            param = {"samples": conf.treatment_bases,
                     "frag_tool": "picard"}))

    elif conf.seq_type.startswith("bam") or conf.seq_type.startswith("sam"):
        if conf.seq_type.split(",")[1].strip() == "pe":
            for target in conf.treatment_targets:
                insert_size = attach_back(workflow, ShellCommand(
                    "{tool} -Xmx5g -XX:ParallelGCThreads={param[threads]} -jar {param[insertsize]} HISTOGRAM_FILE={output[pdf]} I={input[bam]} O={output[insert]} VALIDATION_STRINGENCY=SILENT",
                    tool = "java",
                    input = {"bam": target + "_5M_sort.bam"},
                    output = {"pdf": target + "_picard_insert_5M.pdf", "insert": target + "_insert_metric.5M"},
                    param = {"threads": 4}))
                insert_size.update(param = conf.items("picard"))
            attach_back(workflow, PythonCommand(
                stat_frag_std,
                input = {"insert": [ target + "_insert_metric.5M" for target in conf.treatment_targets ]},
                output = {"json": conf.json_prefix + "_frag.json"},
                param = {"samples": conf.treatment_bases,
                         "frag_tool": "picard"}))
        elif conf.seq_type.split(",")[1].strip() == "se":
            for target in conf.treatment_targets:
                fragment_size = attach_back(workflow, ShellCommand(
                    "{tool} predictd -i {input[bam]} --rfile {param[prefix]}",
                    tool = "macs2",
                    input = {"bam": target + "_5M_sort.bam"},
                    output = {"R": target + "_5M_model.R"},
                    param = {"prefix": target + "_5M"}))
                ## extract standard deviation from MACS2 model.R,
                ## use m, p, and pileup value for standard deviation; mean fragment size is provided(choose the one with highest correlation)
                attach_back(workflow, PythonCommand(
                    stat_frag_std,
                    input = {"r": [ target + "_5M_model.R" for target in conf.treatment_targets ]},
                    output = {"json": conf.json_prefix + "_frag.json", "r": [ target + "_5M_frag_sd.R" for target in conf.treatment_targets ] },
                    param = {"samples": conf.treatment_bases,
                             "frag_tool": "macs2"}))

    elif conf.seq_type.startswith("bed"):
        for target in conf.treatment_targets:
            fragment_size = attach_back(workflow, ShellCommand(
                "{tool} predictd -i {input[bed]} --rfile {param[prefix]}",
                tool = "macs2",
                input = {"bed": target + "_5M.bed"},
                output = {"R": target + "_5M_model.R"},
                param = {"prefix": target + "_5M"}))
            attach_back(workflow, PythonCommand(
                stat_frag_std,
                input = {"r": [ target + "_5M_model.R" for target in conf.treatment_targets ]},
                output = {"json": conf.json_prefix + "_frag.json", "r": [ target + "_5M_frag_sd.R" for target in conf.treatment_targets ] },
                param = {"samples": conf.treatment_bases,
                         "frag_tool": "macs2"}))

    elif conf.seq_type == "se":  ## for single end fastq input and SE, PE bam input, use macs2
        for target in conf.treatment_targets:
            fragment_size = attach_back(workflow, ShellCommand(
                                    "{tool} predictd -i {input[bam]} --rfile {param[prefix]}",
                                    tool = "macs2",
                                    input = {"bam": target + "_5M_sort.bam"},
                                    output = {"R": target + "_5M_model.R"},
                                    param = {"prefix": target + "_5M"}))
        attach_back(workflow, PythonCommand(
            stat_frag_std,
            input = {"r": [ target + "_5M_model.R" for target in conf.treatment_targets ]},
            output = {"json": conf.json_prefix + "_frag.json", "r": [ target + "_5M_frag_sd.R" for target in conf.treatment_targets ] },
            param = {"samples": conf.treatment_bases,
                     "frag_tool": "macs2"}))
    attach_back(workflow, PythonCommand(
        frag_doc,
        input = {"json": conf.json_prefix + "_frag.json", "tex": tex},
        output = {"latex": conf.latex_prefix + "_frag.tex"},
        param = {"reps": len(conf.treatment_pairs),
                 "samples": conf.treatment_bases}))

def _hotspot_on_replicates(workflow, conf):
    # I write a tags.sh in pipeline-scripts to convert BAM files to 1 bp BED files
    # for both SE and PE using all and 5M reads
    ## all reads, need to get 1bp tags density, and 20bp tags density to bigwiggle
    ## use two passes merge peaks and hotspot to estimate peaks number
    ## sometimes wavepeaks don't get peaks evenly distributed, shut down chromosome check
    if conf.seq_type.startswith("bam") or conf.seq_type.startswith("sam"):
        ty = conf.seq_type.split(",")[1].strip().lower()
        kind = ".bam"
    elif conf.seq_type.startswith("bed"):
        ty = "bed"
        kind = "_all.bed"
    else:
        ty = conf.seq_type
        kind = ".bam"

    for i, target in enumerate(conf.treatment_targets):
        ## prepare input for hotspot, get mappable tags and starch into output directory
        if conf.seq_type.startswith("bed"):
            attach_back(workflow,
                ShellCommand(
                    "{tool} {input} {output[starch]} {param[type]}",
                    tool = "tags.sh",
                    input = target + kind,
                    output = {"starch": target + ".bed.starch", "loc_count": target + ".bed.starch.loc_count",
                              "uniq_loc": target + ".bed.starch.uniq_loc"},
                    param = {"type": ty}))
        else:
            attach_back(workflow,
                ShellCommand(
                    "{tool} {input} {output[starch]} {param[type]}",
                    tool = "tags.sh",
                    input = target + kind,
                    output = {"starch": target + ".bed.starch"},
                    param = {"type": ty}))

        ## generate configuration for hotspot v3
        hotspot_conf = attach_back(workflow,
            PythonCommand(spot_conf,
                input = {"spot_conf": token_file,
                         "tag": target + ".bed.starch",
                         "mappable_region": conf.get("hotspot", "mappable_region"),
                         "chrom_info": conf.get("hotspot", "chrom_info")},
                output = {"conf": target + "_runall.tokens.txt",
                          "dir": conf.target_dir},
                param = {"fdrs": "0.01", "K": conf.get("Basis", "read_length"),
                         "species": conf.get("Basis", "species"),
                         "keep_dup": "T"},
                name = "hotspot config"))
        hotspot_conf.param.update(conf.items("hotspot"))

        ## run hotspot v3
        attach_back(workflow, ShellCommand(
            "{tool} {input[pipe]} {input[token]} {output[dir]} {param[spot]}",
            tool = "runhotspot",
            input = {"pipe": pipeline_scripts, "token": target + "_runall.tokens.txt"},
            output = {"dir": conf.target_dir,
                      "spot_peaks_combined": conf.hotspot_reps_both_passes_prefix[i] + ".hotspot.twopass.fdr0.01.merge.pks.bed",
                      "density_starch": target + ".tagdensity.bed.starch",
                      "spot": target + ".spot.out"},
            param = {"spot": "all"}))

        ## hotspot all reads bigwiggle
        attach_back(workflow, ShellCommand(
            "{tool} {input[starch]} > {output[bed]}",
            tool = "unstarch",
            input = {"starch": target + ".tagdensity.bed.starch"},
            output = {"bed": target + ".tagdensity.bed.tmp"}))

        attach_back(workflow,
            ShellCommand(
                '{tool} intersect -a {input} -b {param[chrom_bed]} -f 1.00 > {output}',
                tool="bedtools",
                input=target + ".tagdensity.bed.tmp",
                output=target + ".tagdensity.bed",
                param={'chrom_bed': conf.get("lib", "chrom_bed")},
                name="bed replicate filtering"))

        ## convert to bigwiggle, 20 bp resolution
        attach_back(workflow,
            ShellCommand(
                'cut -f 1,2,3,5 {input} > {input}.tmp && {tool} {input}.tmp {param[chrom_len]} {output}',
                tool = "bedGraphToBigWig",
                input=target + ".tagdensity.bed",
                output=target + "_density.bw",
                param={"chrom_len": conf.get("lib", "chrom_len")}, name="bdg_to_bw"))

    ## Use 5M reads to estimate SPOT
    for i, target in enumerate(conf.treatment_targets):
        ## prepare input for hotspot, get mappable tags and starch into output directory
        if conf.seq_type.startswith("bed"):
            attach_back(workflow,
                ShellCommand(
                    "{tool} {input} {output[starch]} {param[type]}",
                    tool = "tags.sh",
                    input = target + kind,
                    output = {"starch": target + "_5M.bed.starch", "loc_count": target + "_5M.bed.starch.loc_count",
                              "uniq_loc": target + "_5M.bed.starch.uniq_loc"},
                    param = {"type": ty}))
        else:
            attach_back(workflow,
                ShellCommand(
                    "{tool} {input} {output[starch]} {param[type]}",
                    tool = "tags.sh",
                    input = target + kind,
                    output = {"starch": target + "_5M.bed.starch"},
                    param = {"type": ty}))

        ## generate configuration for hotspot v3
        hotspot_conf = attach_back(workflow,
            PythonCommand(spot_conf,
                input = {"spot_conf": token_file,
                         "tag": target + "_5M.bed.starch",
                         "mappable_region": conf.get("hotspot", "mappable_region"),
                         "chrom_info": conf.get("hotspot", "chrom_info")},
                output = {"conf": target + "_runall_5M.tokens.txt",
                          "dir": conf.target_dir},
                param = {"fdrs": "0.01", "K": conf.get("Basis", "read_length"),
                         "species": conf.get("Basis", "species"),
                         "keep_dup": "T"}))
        hotspot_conf.param.update(conf.items("hotspot"))
        ## run hotspot v3
        attach_back(workflow, ShellCommand(
            "{tool} {input[pipe]} {input[token]} {output[dir]}",
            tool = "runhotspot",
            input = {"pipe": pipeline_scripts, "token": target + "_runall_5M.tokens.txt"},
            output = {"dir": conf.target_dir,
                      "spot_peaks_combined": conf.hotspot_reps_both_passes_5M_prefix[i] + ".hotspot.twopass.fdr0.01.merge.pks.bed",
                      "density_starch": target + "_5M.tagdensity.bed.starch",
                      "hotspot":  conf.hotspot_reps_both_passes_5M_prefix[i] + ".twopass.merge150.wgt10.zgt2.wig",
                      "spot": target + "_5M.spot.out"},
            param = {"spot": "5M"}))
        ## 5M reads density from hotspot v3 tag density, 20bp resolution
        ## for correlation evaluation
        attach_back(workflow, ShellCommand(
            "{tool} {input[starch]} > {output[bed]}",
            tool = "unstarch",
            input = {"starch": target + "_5M.tagdensity.bed.starch"},
            output = {"bed": target + "_5M.tagdensity.bed.tmp"}))

        ## For bedGraphToBigwiggle bugs, we need to remove coordinates outlier
        ## filter bdg file to remove over-border coordinates
        ## use 5M bigwiggle to estimate replicates consistency
        attach_back(workflow,
            ShellCommand(
                '{tool} intersect -a {input} -b {param[chrom_bed]} -f 1.00 > {output}',
                tool="bedtools",
                input=target + "_5M.tagdensity.bed.tmp",
                output=target + "_5M.tagdensity.bed",
                param={'chrom_bed': conf.get("lib", "chrom_bed")},
                name="bed replicate filtering"))
        ## convert to bigwiggle
        attach_back(workflow,
            ShellCommand(
                'cut -f 1,2,3,5 {input} > {input}.final && {tool} {input}.final {param[chrom_len]} {output}',
                tool = "bedGraphToBigWig",
                input=target + "_5M.tagdensity.bed",
                output=target + "_5M.bw",
                param={"chrom_len": conf.get("lib", "chrom_len")}, name="bdg_to_bw"))

    ## depend on tags.sh, calculate redundancy
    if conf.seq_type.startswith("bed"):
        attach_back(workflow, PythonCommand(
            stat_redun_census,
            input = {"built_count": [ [target + "_5M.bed.starch.loc_count", target + "_5M.bed.starch.uniq_loc"] for target in conf.treatment_targets ]},
            output = {"json": conf.json_prefix + "_redun.json"}, param = {"samples": conf.treatment_bases, "format": conf.seq_type}))
        attach_back(workflow, PythonCommand(
            redundancy_doc,
            input = {"tex": tex, "json": conf.json_prefix + "_redun.json"},
            output = {"redun": conf.latex_prefix + "redun.tex"},
            param = {"reps": len(conf.treatment_pairs), "samples": conf.treatment_bases}))

def _peaks_reps_preprocess(workflow, conf):
    """
    remove blacklist and outlier for hotspot b
    """
    has_velcro = conf.get("lib", "velcro")  ## mouse genome does not have blacklist available
    for i, target in enumerate(conf.treatment_targets):
        ## remove blacklist for human
        if conf.peakcalltool == "hotspot":
            input_peaks = conf.hotspot_reps_both_passes_5M_prefix[i] + ".twopass.merge150.wgt10.zgt2.wig"
            non_velcro = target + "_5M_velcro_non_overlap_hotspot.bed"
        elif conf.peakcalltool == "macs2":
            input_peaks = target + "_5M_macs2_peaks.bed"
            non_velcro = target + "_5M_macs2_velcro_non_overlap_peaks.bed"

        ## awk and bedClip to remove outlier location from above input
        attach_back(workflow,
            ShellCommand(
                "sed 1d {input} | {tool} '{{if ($2 >= 0 && $2 < $3) print}}' - > {output}",
                tool="awk",
                input = input_peaks,
                output = input_peaks + ".tmp",
                name = "filter bed files outlier location"))

        # prototype used here to do the similar thing on bedclip
        bed_clip = attach_back(workflow,
            ShellCommand(
                template="{tool} {input} {param[chrom_len]} {output}",
                tool="bedClip",
                input=input_peaks + ".tmp",
                output = input_peaks + ".final",
                param={'chrom_len': conf.get_path("lib", "chrom_len")},
                name="bedclip filter"))
        bed_clip.allow_fail = True

        if has_velcro:
            attach_back(workflow,
                ShellCommand(
                    "{tool} -v -a {input[peaks]} -b {input[velcro_peaks_bed]} > {output}",
                    tool="intersectBed",
                    input={"peaks": input_peaks + ".final",
                           "velcro_peaks_bed": conf.get("lib", "velcro")},
                    output = non_velcro + ".final",
                    name = "filter blacklist",
                    param=None))
        else: ## for mouse, just rename to *_5M_velcro_non_overlap_peaks.bed
            attach_back(workflow,
                ShellCommand(
                    "cp {input[peaks]} {output}",
                    tool="cp",
                    input={"peaks": input_peaks + ".final"},
                    output = non_velcro + ".final",
                    name = "No blacklist, not filter blacklist"))

def _peaks_reps_evaluating(workflow, conf):
    ## use 5M reads hotspot filtered regions for evaluation, intersect region ratio in pairwise ways to calculate I/U
    for i in range(len(conf.treatment_targets)):
        for j in range(i+1, len(conf.treatment_targets)):
            if conf.peakcalltool == "hotspot":
                one_rep = conf.treatment_targets[i] + "_5M_velcro_non_overlap_hotspot.bed.final"
                another_rep = conf.treatment_targets[j] + "_5M_velcro_non_overlap_hotspot.bed.final"
            elif conf.peakcalltool == "macs2":
                one_rep = conf.treatment_targets[i] + "_5M_macs2_velcro_non_overlap_peaks.bed.final"
                another_rep =  conf.treatment_targets[j] + "_5M_macs2_velcro_non_overlap_peaks.bed.final"
            attach_back(workflow,
                ShellCommand(
                    "{tool} -a {input[one_rep]} -b {input[another_rep]} > {output}",
                    tool="intersectBed",
                    input = {"one_rep": one_rep,
                             "another_rep": another_rep},
                    output= conf.treatment_targets[i] + "_reps_overlap_" + "rep" + str(i+1) + "rep" + str(j+1) + ".bed",
                    name = "replicates Overlap"))


    ## Implemented by Jim
    ## correlation in prepared union DHS regions(filtered by blacklist), bigWigCorrelate -restrict=testid_reps_union_region.bb testid_treat_rep1_5M.bw testid_treat_rep2_5M.bw
    ## correlation in genome wide, wigCorrelate,  wigCorrelate one.wig two.wig ... n.wig
    for i in range(len(conf.treatment_targets)):
        for j in range(i+1, len(conf.treatment_targets)):
            if conf.peakcalltool == "macs2":
                one_rep = conf.treatment_targets[i] + "_5M_macs2_treat.bw"
                another_rep = conf.treatment_targets[j] + "_5M_macs2_treat.bw"
            elif conf.peakcalltool == "hotspot":
                one_rep = conf.treatment_targets[i] + "_5M.bw"
                another_rep = conf.treatment_targets[j] + "_5M.bw"

            if conf.get("tool", "cor").strip().lower() == "genome":
                attach_back(workflow,
                    ShellCommand(
                        "{tool} {input[one_rep]} {input[another_rep]} 1>{output[cor_score]}",
                        tool="wigCorrelate",
                        input = {"one_rep": one_rep,
                                 "another_rep": another_rep,
                                 "union": conf.get("lib", "filtered_dhs_bb")},
                        output= {"cor_score": conf.treatment_targets[i] + "_reps_" + str(i+1) + str(j+1) + "cor_score"},
                        name = "replicates bigwiggle genome wide correlation"))

            elif conf.get("tool", "cor").strip().lower() == "union":
                attach_back(workflow,
                    ShellCommand(
                        "{tool} -restrict={input[union]} {input[one_rep]} {input[another_rep]} 1>{output[cor_score]}",
                        tool="bigWigCorrelate",
                        input = {"one_rep": one_rep,
                                 "another_rep": another_rep,
                                 "union": conf.get("lib", "filtered_dhs_bb")},
                        output= {"cor_score": conf.treatment_targets[i] + "_reps_" + str(i+1) + str(j+1) + "cor_score"},
                        name = "replicates bigwiggle union DHS correlation"))
    ## peaks replicates json
    bed = [ (conf.treatment_targets[i] + "_reps_overlap_" + "rep" + str(i+1) + "rep" + str(j+1) + ".bed", "Rep %s %s" %(i + 1, j + 1))
            for i in range(len(conf.treatment_targets)) for j in range(i+1, len(conf.treatment_targets)) ]

    cor = [ (conf.treatment_targets[i] + "_reps_" + str(i+1) + str(j+1) + "cor_score", "Rep %s %s" %(i + 1, j + 1))
            for i in range(len(conf.treatment_targets)) for j in range(i+1, len(conf.treatment_targets)) ]

    ## get 5M union hotspot regions for I/U evaluation
    union = attach_back(workflow, ShellCommand(
        "{tool} -u {param[reps]} > {output[union]}",
        tool = "bedops",
        input = {"beds": [ target + "_5M_velcro_non_overlap_hotspot.bed.final" for target in conf.treatment_targets ]},
        output = {"union": conf.prefix + "_hotspot_5M_union.bed"}))
    union.param.update({"reps": " ".join(union.input['beds'])})

    ## get output to json
    attach_back(workflow, PythonCommand(
        stat_reps,
        input = {"5M_overlap": bed, "5M_cor": cor, "union": conf.prefix + "_hotspot_5M_union.bed"},
        output = {"json": conf.json_prefix + "_reps.json"},
        param = {"cor": conf.get("tool", "cor")}
    ))

    ## latex document
    attach_back(workflow, PythonCommand(
        reps_doc,
        input = {"json": conf.json_prefix + "_reps.json", "tex": tex},
        output = {"latex": conf.latex_prefix + "_reps.tex"},
        param = {"samples": conf.treatment_bases}))


def _hotspot_combo(workflow, conf):
    ## all reads combo
    attach_back(workflow,
        ShellCommand(
            "{tool} -u {param[tag_list]} | sort-bed - | starch - > {output}",
            tool = "bedops",
            input = [ target + ".bed.starch" for target in conf.treatment_targets ],
            output = conf.prefix + "_merge_all.bed.starch",
            param = {"tag_list": " ".join([ target + ".bed.starch" for target in conf.treatment_targets ])}))

    ## config for hotspot v3
    hotspot_conf = attach_back(workflow,
        PythonCommand(spot_conf,
            input = {"spot_conf": token_file,
                     "tag": conf.prefix + "_merge_all.bed.starch",
                     "mappable_region": conf.get("hotspot", "mappable_region"),
                     "chrom_info": conf.get("hotspot", "chrom_info")},
            output = {"conf": conf.prefix + "_runall_all.tokens.txt",
                      "dir": conf.target_dir},
            param = {"fdrs": "0.01", "K": conf.get("Basis", "read_length"),
                     "species": conf.get("Basis", "species"),
                     "keep_dup": "T"}))
    hotspot_conf.param.update(conf.items("hotspot"))


    ## run hotspot v3
    attach_back(workflow, ShellCommand(
        "{tool} {input[pipe]} {input[token]} {output[dir]} {param[spot]}",
        tool = "runhotspot",
        input = {"pipe": pipeline_scripts, "token": conf.prefix + "_runall_all.tokens.txt"},
        output = {"dir": conf.target_dir,
                  "spot_peaks_combined": conf.hotspot_merge_both_passes_prefix + "_merge_all.hotspot.twopass.fdr0.01.merge.pks.bed",
                  "density_starch": conf.prefix + "_merge_all.tagdensity.bed.starch",
                  "spot": conf.prefix + "_merge_all.spot.out"},
        param = {"spot": "all"}))

    attach_back(workflow, ShellCommand(
        "{tool} {input[starch]} > {output[bed]}",
        tool = "unstarch",
        input = {"starch": conf.prefix + "_merge_all.tagdensity.bed.starch"},
        output = {"bed": conf.prefix + "_merge_all.tagdensity.bed.tmp"},
        param={'chrom_bed': conf.get("lib", "chrom_bed")},
        name="bed replicate filtering"))

    attach_back(workflow,
        ShellCommand(
            '{tool} intersect -a {input} -b {param[chrom_bed]} -f 1.00 > {output}',
            tool="bedtools",
            input=conf.prefix + "_merge_all.tagdensity.bed.tmp",
            output=conf.prefix + "_merge_all.tagdensity.bed",
            param={'chrom_bed': conf.get("lib", "chrom_bed")},
            name="bed replicate filtering"))

    ## convert to bigwiggle
    attach_back(workflow,
        ShellCommand(
            'cut -f 1,2,3,5 {input} > {input}.final && {tool} {input}.final {param[chrom_len]} {output}',
            tool = "bedGraphToBigWig",
            input=conf.prefix + "_merge_all.tagdensity.bed",
            output=conf.prefix + ".bw",
            param={"chrom_len": conf.get("lib", "chrom_len")}, name="bdg_to_bw"))

def _macs2_on_reps(workflow, conf):
    """
    call peaks by MACS2, optionally
    """
    ## for all reads
    if conf.seq_type.startswith("bed"):
        kind = "_all.bed"
        suffix = "_5M.bed"
    else:
        kind = ".bam"
        suffix = "_5M.bam"

    for target in conf.treatment_targets:
        ## keep all duplicate tags as hotspot
        macs2_on_rep = attach_back(workflow,
            ShellCommand(
                "{tool} callpeak -B -q 0.01 -f {param[format]} -g {param[species]} --keep-dup {param[keep_dup]} --shiftsize={param[shiftsize]} --nomodel -g {param[species]} \
                {param[treat_opt]} -n {param[description]}",
                tool="macs2",
                input={"treat": target + kind},
                output={"peaks": target + "_macs2_all_peaks.bed",
                        "summit": target + "_macs2_all_summits.bed",
                        "treat_bdg": target + "_macs2_all_treat_pileup.bdg",
                        "peaks_xls": target + "_macs2_all_peaks.xls",
                        "control_bdg": target + "_macs2_all_control_lambda.bdg"},
                param={"description": target + "_macs2_all", "keep_dup": "all", "shiftsize": 50, "species": conf.get("macs2", "species")},
                name="macs2_callpeak_rep"))
        macs2_on_rep.param["treat_opt"] = "-t " + macs2_on_rep.input["treat"]
        macs2_on_rep.update(param=conf.items("macs2"))
        if conf.seq_type == "se":
            macs2_on_rep.param["format"] = "BAM"
        elif conf.seq_type == "pe":
            macs2_on_rep.param["format"] = "BAMPE"   ## pair end mode, not compatible with SAM files built-in sampling
        elif conf.seq_type.startswith("bam") or conf.seq_type.startswith("sam"):
            if conf.seq_type.split(",")[1].strip().lower() == "se":
                macs2_on_rep.param["format"] = "BAM"
            elif conf.seq_type.split(",")[1].strip().lower() == "pe":
                macs2_on_rep.param["format"] = "BAMPE"
        elif conf.seq_type.startswith("bed"):
            macs2_on_rep.param["format"] = "BED"

        ## For bedGraphToBigwiggle bugs, we need to remove coordinates outlier
        ## filter bdg file to remove over-border coordinates
        attach_back(workflow,
            ShellCommand(
                '{tool} intersect -a {input} -b {param[chrom_bed]} -f 1.00 > {output}',
                tool="bedtools",
                input=target + "_macs2_all_treat_pileup.bdg",
                output=target + "_macs2_all_treat_pileup.bdg.tmp",
                param={'chrom_bed': conf.get("lib", "chrom_bed")},
                name="bedGraph replicate filtering"))
        bdg2bw_treatrep = attach_back(workflow,
            ShellCommand(
                "{tool} {input} {param[chrom_len]} {output}",
                tool="bedGraphToBigWig",
                input=target + "_macs2_all_treat_pileup.bdg.tmp",
                output=target + "_treat_all.bw",
                param={"chrom_len": conf.get("lib", "chrom_len")}, name="bdg_to_bw"))

    ## for 5M reads
    if conf.seq_type.startswith("bam") or conf.seq_type.startswith("sam"):
        ty = conf.seq_type.split(",")[1].strip().lower()
    else:
        ty = conf.seq_type

    for target in conf.treatment_targets:
        ## keep all duplicate tags as hotspot
        macs2_on_rep = attach_back(workflow,
            ShellCommand(
                "{tool} callpeak -B -q 0.01 -g {param[species]} --keep-dup {param[keep_dup]} --shiftsize={param[shiftsize]} --nomodel -g {param[species]} \
                {param[treat_opt]} -n {param[description]}",
                tool="macs2",
                input={"treat": target + suffix},
                output={"peaks": target + "_5M_macs2_peaks.bed",
                        "summit": target + "_5M_macs2_summits.bed",
                        "treat_bdg": target + "_5M_macs2_treat_pileup.bdg",
                        "peaks_xls": target + "_5M_macs2_peaks.xls",
                        "control_bdg": target + "_5M_macs2_control_lambda.bdg"},
                param={"description": target + "_5M_macs2", "keep_dup": "all", "shiftsize": 50, "species": conf.get("macs2", "species")},
                name="macs2_callpeak_rep"))
        macs2_on_rep.param["treat_opt"] = "-t " + macs2_on_rep.input["treat"]
        macs2_on_rep.update(param=conf.items("macs2"))

        ## SPOT score for MACS2 5M reads
        if conf.seq_type.startswith("bed"):
            attach_back(workflow,
                ShellCommand(
                    "{tool} {input} {output[starch]} {param[type]}",
                    tool = "tags.sh",
                    input = target + suffix,
                    output = {"starch": target + "_5M.bed.starch",
                              "uniq_loc": target + "_5M.bed.starch.uniq_loc",
                              "loc_count": target + "_5M.bed.starch.loc_count"},
                    param = {"type": ty}))
        else:
            attach_back(workflow,
                ShellCommand(
                    "{tool} {input} {output[starch]} {param[type]}",
                    tool = "tags.sh",
                    input = target + suffix,
                    output = {"starch": target + "_5M.bed.starch"},
                    param = {"type": ty}))

        attach_back(workflow, ShellCommand(
            "{tool} {input[starch]} {input[bed]}",
            tool = "macs2_spot.sh",
            input = {"starch": target + "_5M.bed.starch", "bed": target + "_5M_macs2_peaks.bed"},
            output = target + "_5M_macs2_peaks.bed" + ".spot.out"))

        ## For bedGraphToBigwiggle bugs, we need to remove coordinates outlier
        ## filter bdg file to remove over-border coordinates
        attach_back(workflow,
            ShellCommand(
                '{tool} intersect -a {input} -b {param[chrom_bed]} -f 1.00 > {output}',
                tool="bedtools",
                input=target + "_5M_macs2_treat_pileup.bdg",
                output=target + "_5M_macs2_treat_pileup.bdg.tmp",
                param={'chrom_bed': conf.get("lib", "chrom_bed")},
                name="bedGraph replicate filtering"))
        bdg2bw_treatrep = attach_back(workflow,
            ShellCommand(
                "{tool} {input} {param[chrom_len]} {output}",
                tool="bedGraphToBigWig",
                input=target + "_5M_macs2_treat_pileup.bdg.tmp",
                output=target + "_5M_macs2_treat.bw",
                param={"chrom_len": conf.get("lib", "chrom_len")}, name="bdg_to_bw"))

    if conf.seq_type.startswith("bed"):
        attach_back(workflow, PythonCommand(
            stat_redun_census,
            input = {"built_count": [ [target + "_5M.bed.starch.loc_count", target + "_5M.bed.starch.uniq_loc"] for target in conf.treatment_targets ]},
            output = {"json": conf.json_prefix + "_redun.json"}, param = {"samples": conf.treatment_bases, "format": conf.seq_type}))
        attach_back(workflow, PythonCommand(
            redundancy_doc,
            input = {"tex": tex, "json": conf.json_prefix + "_redun.json"},
            output = {"redun": conf.latex_prefix + "redun.tex"},
            param = {"reps": len(conf.treatment_pairs), "samples": conf.treatment_bases}))

def _macs2_on_combo(workflow, conf):
    # merge all treatments into one
    if not conf.seq_type.startswith("bed"):
        merge_bams_treat = ShellCommand(
            "{tool} merge {output[merged]} {param[bams]}",
            tool="samtools",
            input=[ target + ".bam" for target in conf.treatment_targets],
            output={"merged": conf.prefix + "_treatment.bam"})
        merge_bams_treat.param = {"bams": " ".join(merge_bams_treat.input)}
        attach_back(workflow, merge_bams_treat)
        kind = "_treatment.bam"
    else:
        merge_beds_treat = ShellCommand(
            "{tool} {param[bed]} > {output[bed]}",
            tool = "cat",
            input = [ target + ".bed.all" for target in conf.treatment_targets ],
            output = {"bed": conf.prefix + "_treatment.bed"})
        merge_beds_treat.param = {"bed": " ".join(merge_beds_treat.input)}
        attach_back(workflow, merge_beds_treat)
        kind = "_treatment.bed"
    macs2_on_merged = attach_back(workflow, ShellCommand(
        "{tool} callpeak -B -q 0.01 -f {param[format]} --keep-dup {param[keep_dup]} --shiftsize={param[shiftsize]} --nomodel -g {param[species]} \
        {param[treat_opt]} -n {param[description]}",
        tool="macs2",
        input={"merged": conf.prefix + kind},
        output={"peaks": conf.prefix + "_peaks.bed",
                "summit": conf.prefix + "_summits.bed",
                "treat_bdg": conf.prefix + "_treat_pileup.bdg",
                "peaks_xls": conf.prefix + "_peaks.xls",
                "control_bdg": conf.prefix + "_control_lambda.bdg"},
        param={"description": conf.prefix,
               "keep_dup": "all",
               "shiftsize": 50,
               "species": conf.get("macs2",  "species")},
        name="macs2_callpeak_merged"))

    macs2_on_merged.param["treat_opt"] = " -t " + macs2_on_merged.input["merged"]
    macs2_on_merged.update(param=conf.items("macs2"))

    if conf.seq_type == "se":
        macs2_on_merged.param["format"] = "BAM"
    elif conf.seq_type == "pe":
        macs2_on_merged.param["format"] = "BAMPE"   ## pair end mode, not compatible with SAM files built-in sampling
    elif conf.seq_type.startswith("bam") or conf.seq_type.startswith("sam"):
        if conf.seq_type.split(",")[1].strip().lower() == "se":
            macs2_on_merged.param["format"] = "BAM"
        elif conf.seq_type.split(",")[1].strip().lower() == "pe":
            macs2_on_merged.param["format"] = "BAMPE"
    elif conf.seq_type.startswith("bed"):
        macs2_on_merged.param["format"] = "BED"

    ## BedClip
    ## prototype used here to do the similar thing on bedclip
    bed_clip = attach_back(workflow,
        ShellCommand(
            template="{tool} {input} {param[chrom_len]} {output}",
            tool="bedClip",
            input=conf.prefix + "_treat_pileup.bdg",
            output =conf.prefix + "_treat_pileup.bdg.tmp",
            param={'chrom_len': conf.get_path("lib", "chrom_len")},
            name="bedclip filter"))

    bdg2bw_treat = attach_back(workflow,
        ShellCommand(
            "{tool} {input[bdg]} {input[chrom_len]} {output[bw]}",
            tool="bedGraphToBigWig",
            input={"bdg": conf.prefix + "_treat_pileup.bdg.tmp",
                   "chrom_len": conf.get("lib", "chrom_len")},
            output={"bw": conf.prefix + "_macs2_treat_all.bw"},
            name="bdg_to_bw"))

def _peaks_calling_latex(workflow, conf):
    ## peaks number json
    if conf.peakcalltool == "hotspot":
        if len(conf.treatment_pairs) >= 2:
            ## use hotspot b to evaluate 5M reads peaks number, replicates consistency
            ## use peaks d as narrow peaks to evaluate all reads peaks number
            attach_back(workflow, PythonCommand(
                    stat_peaks,
                    input = {"peaks": {"5M_spot": [ conf.hotspot_reps_both_passes_5M_prefix[i] + ".twopass.merge150.wgt10.zgt2.wig" for i in range(len(conf.treatment_pairs)) ],
                                       "all_peaks": [ conf.hotspot_reps_both_passes_prefix[i] + ".hotspot.twopass.fdr0.01.merge.pks.bed" for i in range(len(conf.treatment_pairs)) ],
                                       "combo": conf.hotspot_merge_both_passes_prefix + "_merge_all.hotspot.twopass.fdr0.01.merge.pks.bed" }},
                    output = {"json": conf.json_prefix + "_peaks.json"},
                    param = {"tool": conf.peakcalltool, "samples": conf.treatment_bases},
                    name = "json peaks and hotspot"))
        else:
            attach_back(workflow, PythonCommand(
                    stat_peaks,
                    input = {"peaks": {"5M_spot": [ conf.hotspot_reps_both_passes_5M_prefix[i] + ".twopass.merge150.wgt10.zgt2.wig" for i in range(len(conf.treatment_pairs)) ],
                                       "all_peaks": [ conf.hotspot_reps_both_passes_prefix[i] + ".hotspot.twopass.fdr0.01.merge.pks.bed" for i in range(len(conf.treatment_pairs)) ]}},
                    output = {"json": conf.json_prefix + "_peaks.json"},
                    param = {"tool": conf.peakcalltool, "samples": conf.treatment_bases},
                    name = "json peaks and hotspot"))

    elif conf.peakcalltool == "macs2":
        if len(conf.treatment_pairs) >= 2:
            attach_back(workflow, PythonCommand(
                    stat_peaks,
                    input = {"peaks": {"all_peaks": [ target + "_macs2_all_peaks.bed" for target in conf.treatment_targets ],
                                       "5M_spot": [ target + "_5M_macs2_peaks.bed" for target in conf.treatment_targets ],
                                       "combo": conf.prefix + "_peaks.bed"}},
                    output = {"json": conf.json_prefix + "_peaks.json"},
                    param = {"tool": conf.peakcalltool, "samples": conf.treatment_bases}))
        else:
            attach_back(workflow, PythonCommand(
                    stat_peaks,
                    input = {"peaks": {"all_peaks": [ target + "_macs2_all_peaks.bed" for target in conf.treatment_targets ],
                                       "5M_spot": [ target + "_5M_macs2_peaks.bed" for target in conf.treatment_targets ]}},
                    output = {"json": conf.json_prefix + "_peaks.json"},
                    param = {"tool": conf.peakcalltool, "samples": conf.treatment_bases}))

    ## peaks number latex
    attach_back(workflow, PythonCommand(
        peaks_doc,
        input = {"tex": tex, "json": conf.json_prefix + "_peaks.json"},
        output = {"latex": conf.latex_prefix + "_peaks.tex"},
        param = {"reps": len(conf.treatment_pairs), "samples": conf.treatment_bases},
        name = "peaks number json"))

    if conf.peakcalltool == "hotspot":
        spot = [ target + "_5M.spot.out" for target in conf.treatment_targets ]
    elif conf.peakcalltool == "macs2":
        spot = [ target + "_5M_macs2_peaks.bed" + ".spot.out" for target in conf.treatment_targets ]

    ## 5M reads, calculate the merged two passes and peaks regions number
    ## 5M reads for macs2 optionally
    attach_back(workflow, PythonCommand(
        stat_spot_on_replicates,
        input = {"spot_files": spot},
        output = {"json": conf.json_prefix + "_sample_spot_5M.json"},
        name = "json spot",
        param = {"samples": conf.treatment_bases}))

    attach_back(workflow, PythonCommand(
        spot_doc,
        input = {"json": conf.json_prefix + "_sample_spot_5M.json",
                 "tex": tex},
        output = {"latex": conf.latex_prefix + "_spot.tex"},
        name = "doc spot 5M",
        param = {"samples": conf.treatment_bases, "reps": len(conf.treatment_pairs)}))

def _promoter_overlap(workflow, conf):
    """ use bedtools to get promotor peaks enrichment,
    Genome Promotor: cutoff 50% promotor overlaps with mappability regions bases number
    Peaks Promotor: cutoff 50% promotor overlaps with peaks regions bases number
    use 5M reads hotspot b to evaluate promotor overlap
    """
    ## use hotspot mappable_region for both hotspot and macs2 peak calling
    attach_back(workflow,
        ShellCommand(
            "{tool} -e -{param[percentage]} {input[ref]} {input[map]} | wc -l > {output[count]}",
            tool = "bedops",
            input = {"ref":conf.get("lib", "tss"), "map": conf.get("hotspot", "mappable_region")},
            output = {"count": conf.prefix + "_promotor.count"},
            param = {"percentage": "50%"}))

    ## peaks region evaluation
    for i, target in enumerate(conf.treatment_targets):
        if conf.peakcalltool == "hotspot":
            peaks = target + "_5M_velcro_non_overlap_hotspot.bed.final"
        elif conf.peakcalltool == "macs2":
            peaks = target + "_5M_macs2_velcro_non_overlap_peaks.bed.final"
        attach_back(workflow,
            ShellCommand(
                "{tool} -e -{param[percentage]} {input[ref]} {input[promotor]} | wc -l > {output[count]} && wc -l {input[ref]} > {output[peaks]}",
                tool = "bedops",
                input = {"ref": peaks,
                         "promotor": conf.get("lib", "tss")},
                output = {"count": target + "_5M_promotor_overlap.count",
                          "peaks": target + "_peaks.count"},
                param = {"percentage": "50%"},
                name = "promotor overlaps"))
    ## promotor json
    attach_back(workflow, PythonCommand(
        stat_promotor,
        input = {"peaks_promotor": [ target + "_5M_promotor_overlap.count" for target in conf.treatment_targets ],
                 "peaks": [ target + "_peaks.count" for target in conf.treatment_targets ],
                 "promotor": conf.prefix + "_promotor.count",
                 "mappable": conf.get("hotspot", "mappable_region")},
        output = {"json": conf.json_prefix + "_promotor.json"},
        param = {"samples": conf.treatment_bases}))

    ## promotor latex document
    attach_back(workflow, PythonCommand(
        promotor_doc,
        input = {"json": conf.json_prefix + "_promotor.json",
                 "tex": tex},
        output = {"latex": conf.latex_prefix + "_promotor.tex"},
        param = {"samples": conf.treatment_bases,
                 "reps": len(conf.treatment_pairs)}))

def _conservation(workflow, conf):
    ## use 5M reads hotspot b for evaluation of conservation
    for target in conf.treatment_targets:
        ## get non-promotor peaks regions
        if conf.peakcalltool == "hotspot":
            peaks = target + "_5M_velcro_non_overlap_hotspot.bed.final"
            non_promotor = target + "_5M_non-promotor_peaks_bed"
            phascon = target + "_100bp_phastcon.score"
        elif conf.peakcalltool == "macs2":
            peaks = target + "_5M_macs2_velcro_non_overlap_peaks.bed.final"
            non_promotor = target + "_macs2_5M_non-promotor_peaks_bed"
            phascon = target + "_macs2_100bp_phastcon.score"
        attach_back(workflow,
            ShellCommand(
                "{tool} -v -a {input[peaks]} -b {input[tss]} > {output}",
                tool = "intersectBed",
                input = {"peaks": peaks, "tss": conf.get("lib", "tss")},
                output = non_promotor,
                name = "Get non-promotor peaks regions",
                param = None))

        get_top_peaks = attach_back(workflow,
            ShellCommand(
                "{tool} -r -g -k 5 {input} | head -n {param[peaks]} > {output}",
                tool="sort",
                input=non_promotor,
                output=non_promotor + ".top1000",
                param={'peaks': 1000}, name="top summits for conservation"))
        get_top_peaks.update(param=conf.items('conservation'))

        ## get summits is implemented in conservation_average.py
        attach_back(workflow,
            ShellCommand(
                "{tool} -w {param[width]} -d {input[phastcon_db]} {input[bed]} 1>{output}",
                tool = "conservation_average.py",
                input= {"phastcon_db": conf.get_path("lib", "phast"),
                        "bed": non_promotor + ".top1000"},
                output = phascon,
                param = {"width": 100}))

    ## Phastcon json
    attach_back(workflow,
        PythonCommand(stat_conserv,
            input = {"phastcon":[ target + "_100bp_phastcon.score" if conf.peakcalltool == "hotspot" else target + "_macs2_100bp_phastcon.score"
                      for target in conf.treatment_targets ]},
            output = {"json": conf.json_prefix + "_conserv.json"},
            param = {"sample": conf.treatment_bases}))

    ## Phaston latex
    attach_back(workflow,
        PythonCommand(conserv_doc,
            input = {"tex": tex, "json": conf.json_prefix + "_conserv.json"},
            output = {"latex": conf.latex_prefix + "_conserv.tex"},
            param = {"reps": len(conf.treatment_pairs),
                     "sample": conf.treatment_bases}))

def _union_DHS_overlap(workflow, conf):
    """ use hotspot d narrow peaks for DHS evaluation """
    for i, target in enumerate(conf.treatment_targets):
        if conf.peakcalltool == "hotspot":
            peaks = conf.hotspot_reps_both_passes_5M_prefix[i] + ".hotspot.twopass.fdr0.01.merge.pks.bed"
            output = target + "_DHS_overlap_peaks_bed"
        elif conf.peakcalltool == "macs2":
            peaks = target + "_5M_macs2_velcro_non_overlap_peaks.bed.final"
            output = target + "_macs2_DHS_overlap_peaks_bed"

        attach_back(workflow,
            ShellCommand(
                "{tool} -wa -u  \
                -a {input[pks_spot_bed]} -b {input[DHS_peaks_bed]} > {output}",
                tool="intersectBed",
                input={"pks_spot_bed": peaks,
                       "DHS_peaks_bed": conf.get("lib", "dhs")},
                output = output,
                name = "Write out DHS overlap BED"))
    attach_back(workflow, PythonCommand(
        stat_dhs,
        input={"dhs_peaks": [ target + "_DHS_overlap_peaks_bed" if conf.peakcalltool == "hotspot" else target + "_macs2_DHS_overlap_peaks_bed"
                            for target in conf.treatment_targets ],
               "pks_spot_bed": [ conf.hotspot_reps_both_passes_5M_prefix[i] + ".hotspot.twopass.fdr0.01.merge.pks.bed" if conf.peakcalltool == "hotspot" else target + "_5M_macs2_velcro_non_overlap_peaks.bed.final"
                                for i, target in enumerate(conf.treatment_targets) ]},
        output = {"json": conf.json_prefix + "_dhs.json"},
        param= {"samples":conf.treatment_bases},
        name="DHS summary"))
    attach_back(workflow, PythonCommand(
        DHS_doc,
        input = {"tex": tex, "json": conf.json_prefix + "_dhs.json"},
        output = {"latex": conf.latex_prefix + "_dhs.tex"},
        param = {"reps": len(conf.treatment_pairs), "samples": conf.treatment_bases}))

def report(workflow, conf):
    collector = []

    ## begin doc and table header
    attach_back(workflow, PythonCommand(
        begin_doc,
        input = tex,
        output = {"begin": conf.latex_prefix + "_begin.tex",
                  "header": conf.latex_prefix + "_header.tex"},
        param = {"rep": len(conf.treatment_pairs), "id": underline_to_space(conf.id)}))

    collector.append(conf.latex_prefix + "_begin.tex")
    collector.append(conf.latex_prefix + "_header.tex")

    # end doc
    attach_back(workflow, PythonCommand(
        end_doc,
        input = {"tex": tex},
        output = {"table_end": conf.latex_prefix + "_table_end.tex",
                  "doc_end": conf.latex_prefix + "_doc_end.tex"},
        param = {}))
    if not conf.seq_type.startswith("bed"):
        collector.append(conf.latex_prefix + "raw.tex")
        collector.append(conf.latex_prefix + "len.tex")
        collector.append(conf.latex_prefix + "seq_quality.tex")
        collector.append(conf.latex_prefix + "mapping.tex")
    collector.append(conf.latex_prefix + "_nsc.tex")
    collector.append(conf.latex_prefix + "_rsc.tex")
    collector.append(conf.latex_prefix + "_frag.tex")
    collector.append(conf.latex_prefix + "redun.tex")
    collector.append(conf.latex_prefix + "_peaks.tex")
    collector.append(conf.latex_prefix + "_spot.tex")

    if len(conf.treatment_pairs) >= 2:
        collector.append(conf.latex_prefix + "_reps.tex")

    ## not needed for ENCODE
#    collector.append(conf.latex_prefix + "_promotor.tex")
#    collector.append(conf.latex_prefix + "_conserv.tex")
    collector.append(conf.latex_prefix + "_dhs.tex")
    collector.append(conf.latex_prefix + "_table_end.tex")
    collector.append(conf.latex_prefix + "_doc_end.tex")

    integrate_doc = attach_back(workflow, ShellCommand(
            "cat {param[input]} > {output[tex]} && {tool} -output-directory {output[dir]} -jobname={param[name]} {output[tex]}\
             && {tool} -output-directory {output[dir]} -jobname={param[name]} {output[tex]}",
            tool="pdflatex",
            input= collector,
            # output[pdf] should use "conf.prefix" to have the absolute path
            output={"tex": conf.prefix + "_report.tex", "dir": conf.target_dir, "pdf": conf.prefix + "_report.pdf"},
            # param[name] should use "conf.id" to avoid using absolute path
            param={"name": conf.id + "_report"},
            name="report"))

    integrate_doc.param.update({"input": " ".join(integrate_doc.input)})

def prepare_clean_up(workflow, conf):
    """
    package all the necessary results and delete temporary files
    preserve bed, bigwiggle, starch, bam and pdf report
    """
    p_list = ['*.bam', '*.xls', '*_summits.bed', '*_peaks.bed', '*.bw', '*treat*.bed.starch',  "*final",
              '*.R', 'json', '*_treat*-both-passes', '*latex', "*.conf", "*report.tex", "*report.pdf"]

    p_pattern = [os.path.join(conf.target_dir, p) for p in p_list]

    final_dir = conf.target_dir + '/dataset_' + conf.id
    attach_back(workflow,
        ShellCommand("if [ ! -d '{output}' ]; then mkdir -p {output}; fi",
            output=final_dir))

    for pf in p_pattern:
        if not glob(pf):
            print(pf)
            continue
        move = attach_back(workflow,
            ShellCommand('mv {param[preserve_files]} {output[dir]} \n# Pattern: {param[p_pattern]}',
                output={"dir": final_dir},
                param={"preserve_files": " ".join(glob(pf)),
                       "p_pattern": pf}, ))
        move.allow_fail = True

def prepare_purge(workflow, conf):
    d_pattern = os.path.join(conf.target_dir, "*")

    for df in glob(d_pattern):
        if "dataset" in df:
            print(df)
            continue
        deleted = attach_back(workflow,
            ShellCommand("rm -r {param[deleted_files]}",
                param={"deleted_files": " ".join(glob(df))}))
        deleted.allow_fail = True

def create_workflow(args, conf):
    workflow = Workflow(name="Main")
    bld = GCAPBuilder(workflow, conf)

    if args.sub_command == "clean":
        ## not implemented yet
        bld.build(prepare_clean_up)
        return workflow

    if args.sub_command == "purge":
        bld.build(prepare_purge)
        return workflow

    if args.skip_step:
        skipped_steps = [int(i) for i in args.skip_step.split(",")]
    else:
        skipped_steps = []

    step_checker = StepChecker(args.start_step, args.end_step, skipped_steps)

    have_treat_reps = len(conf.treatment_pairs) >= 2 ## replicates
    has_dhs = conf.get("lib", "dhs")
    need_run = step_checker.need_run

    bld.attach_back(ShellCommand(
        "if [ ! -d '{output}' ]; then mkdir -p {output}; fi",
        output=conf.target_dir))

    if need_run(1):
        ## input fastq, sample 100k and fastqc,
        ## output fastqc report
        bld.build(_fastqc)

    if need_run(2) and not re.search(r"\.bam",conf.treatment_bam[0]): ## if remapping, use this options, not finished yet
        bld.build(_lib_contamination)

    if need_run(3): ## if bam, skip mapping part, only do statistics through sam files
        bld.build(_reads_mapping) ## mapping and convert format to sorted bam

    if need_run(4):
        bld.build(_library_complexity)

    if need_run(5):
        bld.build(_strand_cor)

    if need_run(6):
        bld.build(_fragment_size)

    if need_run(7):
        if conf.peakcalltool == "hotspot":
            bld.build(_hotspot_on_replicates)
        elif conf.peakcalltool == "macs2":
            bld.build(_macs2_on_reps)
        bld.build(_peaks_reps_preprocess)
        if have_treat_reps:
            bld.build(_peaks_reps_evaluating)
            if conf.peakcalltool == "hotspot":
                bld.build(_hotspot_combo)
            elif conf.peakcalltool == "macs2":
                bld.build(_macs2_on_combo)
        bld.build(_peaks_calling_latex)

#    ## these two criteria, indirectly, abandon
#    if need_run(8):
#        bld.build(_promoter_overlap)
#    if need_run(9):
#        bld.build(_conservation)
    if has_dhs and need_run(8):
        bld.build(_union_DHS_overlap)
    if need_run(9):
        bld.build(report)
    return workflow

def main(args=None):
    args = parse_args(args)
    print("Arguments:", args)

    if args.sub_command in ["run", "clean", "purge"]:
        conf = Conf(args.config)
        workflow = create_workflow(args, conf)

        workflow.set_option(
            verbose_level=args.verbose_level,
            dry_run_mode=args.dry_run,
            resume=args.resume,
            allow_dangling=args.allow_dangling)

        workflow.invoke()

    elif args.sub_command == "batch":
        with open(args.batch_config) as batch_file:
            for a_conf in batch_file:
                a_conf = a_conf.strip()
                conf = Conf(a_conf)
                workflow = create_workflow(args, conf)
                workflow.set_option(
                    verbose_level=args.verbose_level,
                    dry_run_mode=args.dry_run,
                    resume=args.resume,
                    allow_dangling=args.allow_dangling)
                workflow.invoke()

if __name__ == "__main__":
    main()
